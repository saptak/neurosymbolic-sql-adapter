<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Python: module adapters.confidence_estimator</title>
</head><body>

<table class="heading">
<tr class="heading-text decor">
<td class="title">&nbsp;<br><strong class="title"><a href="adapters.html" class="white">adapters</a>.confidence_estimator</strong></td>
<td class="extra"><a href=".">index</a><br><a href="file:/Users/saptak/code/neurosymbolic-sql-adapter/src/adapters/confidence_estimator.py">/Users/saptak/code/neurosymbolic-sql-adapter/src/adapters/confidence_estimator.py</a></td></tr></table>
    <p><span class="code">Confidence&nbsp;Estimator<br>
&nbsp;<br>
Uncertainty&nbsp;quantification&nbsp;module&nbsp;for&nbsp;neurosymbolic&nbsp;SQL&nbsp;generation.<br>
Estimates&nbsp;confidence&nbsp;scores&nbsp;for&nbsp;generated&nbsp;SQL&nbsp;queries&nbsp;and&nbsp;symbolic&nbsp;reasoning.</span></p>
<p>
<table class="section">
<tr class="decor pkg-content-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Modules</strong></td></tr>
    
<tr><td class="decor pkg-content-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><table><tr><td class="multicolumn"><a href="torch.nn.functional.html">torch.nn.functional</a><br>
<a href="logging.html">logging</a><br>
</td><td class="multicolumn"><a href="torch.nn.html">torch.nn</a><br>
<a href="numpy.html">numpy</a><br>
</td><td class="multicolumn"><a href="torch.html">torch</a><br>
</td><td class="multicolumn"></td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor index-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Classes</strong></td></tr>
    
<tr><td class="decor index-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><dl>
<dt class="heading-text"><a href="builtins.html#object">builtins.object</a>
</dt><dd>
<dl>
<dt class="heading-text"><a href="adapters.confidence_estimator.html#ConfidenceOutput">ConfidenceOutput</a>
</dt></dl>
</dd>
<dt class="heading-text"><a href="enum.html#Enum">enum.Enum</a>(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="adapters.confidence_estimator.html#ConfidenceMethod">ConfidenceMethod</a>
</dt></dl>
</dd>
<dt class="heading-text"><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="adapters.confidence_estimator.html#AttentionBasedEstimator">AttentionBasedEstimator</a>
</dt><dt class="heading-text"><a href="adapters.confidence_estimator.html#ConfidenceEstimator">ConfidenceEstimator</a>
</dt><dt class="heading-text"><a href="adapters.confidence_estimator.html#EntropyConfidenceEstimator">EntropyConfidenceEstimator</a>
</dt><dt class="heading-text"><a href="adapters.confidence_estimator.html#MonteCarloDropoutEstimator">MonteCarloDropoutEstimator</a>
</dt><dt class="heading-text"><a href="adapters.confidence_estimator.html#SymbolicConsistencyEstimator">SymbolicConsistencyEstimator</a>
</dt><dt class="heading-text"><a href="adapters.confidence_estimator.html#TemperatureScalingEstimator">TemperatureScalingEstimator</a>
</dt></dl>
</dd>
</dl>
 <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="AttentionBasedEstimator">class <strong>AttentionBasedEstimator</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#AttentionBasedEstimator">AttentionBasedEstimator</a>(hidden_dim:&nbsp;int&nbsp;=&nbsp;512)<br>
&nbsp;<br>
Confidence&nbsp;estimation&nbsp;based&nbsp;on&nbsp;attention&nbsp;patterns<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="adapters.confidence_estimator.html#AttentionBasedEstimator">AttentionBasedEstimator</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="AttentionBasedEstimator-__init__"><strong>__init__</strong></a>(self, hidden_dim: int = 512)</dt><dd><span class="code">Initialize&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-forward"><strong>forward</strong></a>(self, attention_weights: torch.Tensor, hidden_states: torch.Tensor) -&gt; torch.Tensor</dt><dd><span class="code">Estimate&nbsp;confidence&nbsp;based&nbsp;on&nbsp;attention&nbsp;patterns<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;attention_weights:&nbsp;Attention&nbsp;weights&nbsp;[batch_size,&nbsp;num_heads,&nbsp;seq_len,&nbsp;seq_len]<br>
&nbsp;&nbsp;&nbsp;&nbsp;hidden_states:&nbsp;Hidden&nbsp;states&nbsp;[batch_size,&nbsp;seq_len,&nbsp;hidden_dim]<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Confidence&nbsp;scores&nbsp;[batch_size,&nbsp;seq_len]</span></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__annotations__</strong> = {}</dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="AttentionBasedEstimator-__call__"><strong>__call__</strong></a> = <a href="#AttentionBasedEstimator-_wrapped_call_impl">_wrapped_call_impl</a>(self, *args, **kwargs)</dt></dl>

<dl><dt><a name="AttentionBasedEstimator-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><span class="code">Implement&nbsp;delattr(self,&nbsp;name).</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><span class="code">Default&nbsp;dir()&nbsp;implementation.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-__getattr__"><strong>__getattr__</strong></a>(self, name: str) -&gt; Union[torch.Tensor, ForwardRef('Module')]</dt><dd><span class="code">#&nbsp;It&nbsp;is&nbsp;crucial&nbsp;that&nbsp;the&nbsp;return&nbsp;type&nbsp;is&nbsp;not&nbsp;annotated&nbsp;as&nbsp;`Any`,&nbsp;otherwise&nbsp;type&nbsp;checking<br>
#&nbsp;on&nbsp;`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;and&nbsp;all&nbsp;its&nbsp;subclasses&nbsp;is&nbsp;largely&nbsp;disabled&nbsp;as&nbsp;a&nbsp;result.&nbsp;See:<br>
#&nbsp;<a href="https://github.com/pytorch/pytorch/pull/115074">https://github.com/pytorch/pytorch/pull/115074</a></span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-__getstate__"><strong>__getstate__</strong></a>(self)</dt><dd><span class="code">Helper&nbsp;for&nbsp;pickle.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-__setattr__"><strong>__setattr__</strong></a>(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="AttentionBasedEstimator-add_module"><strong>add_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-apply"><strong>apply</strong></a>(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</dt><dd><span class="code">Apply&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#AttentionBasedEstimator-children">children</a>()``)&nbsp;as&nbsp;well&nbsp;as&nbsp;self.<br>
&nbsp;<br>
Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;@torch.no_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#AttentionBasedEstimator-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#AttentionBasedEstimator-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-bfloat16"><strong>bfloat16</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``bfloat16``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-buffers"><strong>buffers</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#AttentionBasedEstimator-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#AttentionBasedEstimator-type">type</a>(buf),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-children"><strong>children</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-compile"><strong>compile</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Compile&nbsp;this&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;using&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
This&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;`__call__`&nbsp;method&nbsp;is&nbsp;compiled&nbsp;and&nbsp;all&nbsp;arguments&nbsp;are&nbsp;passed&nbsp;as-is<br>
to&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
See&nbsp;:func:`torch.compile`&nbsp;for&nbsp;details&nbsp;on&nbsp;the&nbsp;arguments&nbsp;for&nbsp;this&nbsp;function.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-cpu"><strong>cpu</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-cuda"><strong>cuda</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-double"><strong>double</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-eval"><strong>eval</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#AttentionBasedEstimator-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#AttentionBasedEstimator-eval">eval</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-extra_repr"><strong>extra_repr</strong></a>(self) -&gt; str</dt><dd><span class="code">Return&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;re-implement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-float"><strong>float</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``float``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-get_buffer"><strong>get_buffer</strong></a>(self, target: str) -&gt; 'Tensor'</dt><dd><span class="code">Return&nbsp;the&nbsp;buffer&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;The&nbsp;buffer&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-get_extra_state"><strong>get_extra_state</strong></a>(self) -&gt; Any</dt><dd><span class="code">Return&nbsp;any&nbsp;extra&nbsp;state&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict.<br>
&nbsp;<br>
Implement&nbsp;this&nbsp;and&nbsp;a&nbsp;corresponding&nbsp;:func:`set_extra_state`&nbsp;for&nbsp;your&nbsp;module<br>
if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state.&nbsp;This&nbsp;function&nbsp;is&nbsp;called&nbsp;when&nbsp;building&nbsp;the<br>
module's&nbsp;`<a href="#AttentionBasedEstimator-state_dict">state_dict</a>()`.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;extra&nbsp;state&nbsp;should&nbsp;be&nbsp;picklable&nbsp;to&nbsp;ensure&nbsp;working&nbsp;serialization<br>
of&nbsp;the&nbsp;state_dict.&nbsp;We&nbsp;only&nbsp;provide&nbsp;backwards&nbsp;compatibility&nbsp;guarantees<br>
for&nbsp;serializing&nbsp;Tensors;&nbsp;other&nbsp;objects&nbsp;may&nbsp;break&nbsp;backwards&nbsp;compatibility&nbsp;if<br>
their&nbsp;serialized&nbsp;pickled&nbsp;form&nbsp;changes.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="builtins.html#object">object</a>:&nbsp;Any&nbsp;extra&nbsp;state&nbsp;to&nbsp;store&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-get_parameter"><strong>get_parameter</strong></a>(self, target: str) -&gt; 'Parameter'</dt><dd><span class="code">Return&nbsp;the&nbsp;parameter&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;Parameter<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Parameter:&nbsp;The&nbsp;Parameter&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``nn.Parameter``</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-get_submodule"><strong>get_submodule</strong></a>(self, target: str) -&gt; 'Module'</dt><dd><span class="code">Return&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(16,&nbsp;33,&nbsp;kernel_size=(3,&nbsp;3),&nbsp;stride=(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(in_features=100,&nbsp;out_features=200,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;which&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;check&nbsp;whether&nbsp;or&nbsp;not&nbsp;we&nbsp;have&nbsp;the&nbsp;``linear``&nbsp;submodule,&nbsp;we<br>
would&nbsp;call&nbsp;``<a href="#AttentionBasedEstimator-get_submodule">get_submodule</a>("net_b.linear")``.&nbsp;To&nbsp;check&nbsp;whether<br>
we&nbsp;have&nbsp;the&nbsp;``conv``&nbsp;submodule,&nbsp;we&nbsp;would&nbsp;call<br>
``<a href="#AttentionBasedEstimator-get_submodule">get_submodule</a>("net_b.net_c.conv")``.<br>
&nbsp;<br>
The&nbsp;runtime&nbsp;of&nbsp;``get_submodule``&nbsp;is&nbsp;bounded&nbsp;by&nbsp;the&nbsp;degree<br>
of&nbsp;module&nbsp;nesting&nbsp;in&nbsp;``target``.&nbsp;A&nbsp;query&nbsp;against<br>
``named_modules``&nbsp;achieves&nbsp;the&nbsp;same&nbsp;result,&nbsp;but&nbsp;it&nbsp;is&nbsp;O(N)&nbsp;in<br>
the&nbsp;number&nbsp;of&nbsp;transitive&nbsp;modules.&nbsp;So,&nbsp;for&nbsp;a&nbsp;simple&nbsp;check&nbsp;to&nbsp;see<br>
if&nbsp;some&nbsp;submodule&nbsp;exists,&nbsp;``get_submodule``&nbsp;should&nbsp;always&nbsp;be<br>
used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;The&nbsp;submodule&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-half"><strong>half</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-ipu"><strong>ipu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;IPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;IPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-load_state_dict"><strong>load_state_dict</strong></a>(
    self,
    state_dict: collections.abc.Mapping[str, typing.Any],
    strict: bool = True,
    assign: bool = False
)</dt><dd><span class="code">Copy&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into&nbsp;this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.<br>
&nbsp;<br>
If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;:attr:`assign`&nbsp;is&nbsp;``True``&nbsp;the&nbsp;optimizer&nbsp;must&nbsp;be&nbsp;created&nbsp;after<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;call&nbsp;to&nbsp;:attr:`load_state_dict`&nbsp;unless<br>
&nbsp;&nbsp;&nbsp;&nbsp;:func:`~torch.__future__.get_swap_module_params_on_conversion`&nbsp;is&nbsp;``True``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;&nbsp;&nbsp;&nbsp;assign&nbsp;(bool,&nbsp;optional):&nbsp;When&nbsp;set&nbsp;to&nbsp;``False``,&nbsp;the&nbsp;properties&nbsp;of&nbsp;the&nbsp;tensors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;current&nbsp;module&nbsp;are&nbsp;preserved&nbsp;whereas&nbsp;setting&nbsp;it&nbsp;to&nbsp;``True``&nbsp;preserves<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;properties&nbsp;of&nbsp;the&nbsp;Tensors&nbsp;in&nbsp;the&nbsp;state&nbsp;dict.&nbsp;The&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exception&nbsp;is&nbsp;the&nbsp;``requires_grad``&nbsp;field&nbsp;of&nbsp;:class:`~torch.nn.Parameter`s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;which&nbsp;the&nbsp;value&nbsp;from&nbsp;the&nbsp;module&nbsp;is&nbsp;preserved.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;any&nbsp;keys&nbsp;that&nbsp;are&nbsp;expected<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;missing&nbsp;from&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;keys&nbsp;that&nbsp;are&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expected&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;present&nbsp;in&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;a&nbsp;parameter&nbsp;or&nbsp;buffer&nbsp;is&nbsp;registered&nbsp;as&nbsp;``None``&nbsp;and&nbsp;its&nbsp;corresponding&nbsp;key<br>
&nbsp;&nbsp;&nbsp;&nbsp;exists&nbsp;in&nbsp;:attr:`state_dict`,&nbsp;:meth:`load_state_dict`&nbsp;will&nbsp;raise&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;``RuntimeError``.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-modules"><strong>modules</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#AttentionBasedEstimator-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-mtia"><strong>mtia</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;MTIA.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;MTIA&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-named_buffers"><strong>named_buffers</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool,&nbsp;optional):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;buffers&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#AttentionBasedEstimator-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-named_children"><strong>named_children</strong></a>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#AttentionBasedEstimator-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-named_modules"><strong>named_modules</strong></a>(
    self,
    memo: Optional[set['Module']] = None,
    prefix: str = '',
    remove_duplicate: bool = True
)</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;memo:&nbsp;a&nbsp;memo&nbsp;to&nbsp;store&nbsp;the&nbsp;set&nbsp;of&nbsp;modules&nbsp;already&nbsp;added&nbsp;to&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix:&nbsp;a&nbsp;prefix&nbsp;that&nbsp;will&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate:&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;module&nbsp;instances&nbsp;in&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;not<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#AttentionBasedEstimator-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-named_parameters"><strong>named_parameters</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#AttentionBasedEstimator-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-parameters"><strong>parameters</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#AttentionBasedEstimator-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#AttentionBasedEstimator-type">type</a>(param),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_backward_hook"><strong>register_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;deprecated&nbsp;in&nbsp;favor&nbsp;of&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.register_full_backward_hook`&nbsp;and<br>
the&nbsp;behavior&nbsp;of&nbsp;this&nbsp;function&nbsp;will&nbsp;change&nbsp;in&nbsp;future&nbsp;versions.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_buffer"><strong>register_buffer</strong></a>(
    self,
    name: str,
    tensor: Optional[torch.Tensor],
    persistent: bool = True
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;module's&nbsp;state.&nbsp;Buffers,&nbsp;by<br>
default,&nbsp;are&nbsp;persistent&nbsp;and&nbsp;will&nbsp;be&nbsp;saved&nbsp;alongside&nbsp;parameters.&nbsp;This<br>
behavior&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;setting&nbsp;:attr:`persistent`&nbsp;to&nbsp;``False``.&nbsp;The<br>
only&nbsp;difference&nbsp;between&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;and&nbsp;a&nbsp;non-persistent&nbsp;buffer<br>
is&nbsp;that&nbsp;the&nbsp;latter&nbsp;will&nbsp;not&nbsp;be&nbsp;a&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
:attr:`state_dict`.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor&nbsp;or&nbsp;None):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.&nbsp;If&nbsp;``None``,&nbsp;then&nbsp;operations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;run&nbsp;on&nbsp;buffers,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;buffer&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the&nbsp;module's&nbsp;:attr:`state_dict`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;(bool):&nbsp;whether&nbsp;the&nbsp;buffer&nbsp;is&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:attr:`state_dict`.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#AttentionBasedEstimator-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_forward_hook"><strong>register_forward_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False,
    always_call: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``False``&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but&nbsp;it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on<br>
forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after&nbsp;:func:`forward`&nbsp;is&nbsp;called.&nbsp;The&nbsp;hook<br>
should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``True``,&nbsp;the&nbsp;forward&nbsp;hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
``kwargs``&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function&nbsp;and&nbsp;be&nbsp;expected&nbsp;to&nbsp;return&nbsp;the<br>
output&nbsp;possibly&nbsp;modified.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;always_call&nbsp;(bool):&nbsp;If&nbsp;``True``&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;run&nbsp;regardless&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;whether&nbsp;an&nbsp;exception&nbsp;is&nbsp;raised&nbsp;while&nbsp;calling&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
&nbsp;<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;false&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a&nbsp;single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the<br>
hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple&nbsp;if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned<br>
(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the<br>
following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;true,&nbsp;the&nbsp;forward&nbsp;pre-hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.&nbsp;And&nbsp;if&nbsp;the&nbsp;hook&nbsp;modifies&nbsp;the<br>
input,&nbsp;both&nbsp;the&nbsp;args&nbsp;and&nbsp;kwargs&nbsp;should&nbsp;be&nbsp;returned.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have<br>
the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;a&nbsp;tuple&nbsp;of&nbsp;modified&nbsp;input&nbsp;and&nbsp;kwargs<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the&nbsp;kwargs<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_full_backward_hook"><strong>register_full_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;a&nbsp;module<br>
are&nbsp;computed,&nbsp;i.e.&nbsp;the&nbsp;hook&nbsp;will&nbsp;execute&nbsp;if&nbsp;and&nbsp;only&nbsp;if&nbsp;the&nbsp;gradients&nbsp;with<br>
respect&nbsp;to&nbsp;module&nbsp;outputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following<br>
signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple(Tensor)&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;are&nbsp;tuples&nbsp;that&nbsp;contain&nbsp;the&nbsp;gradients<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs&nbsp;respectively.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;:attr:`grad_input`&nbsp;will&nbsp;only&nbsp;correspond&nbsp;to&nbsp;the&nbsp;inputs&nbsp;given<br>
as&nbsp;positional&nbsp;arguments&nbsp;and&nbsp;all&nbsp;kwarg&nbsp;arguments&nbsp;are&nbsp;ignored.&nbsp;Entries<br>
in&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for&nbsp;all&nbsp;non-Tensor<br>
arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;or&nbsp;outputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_full_backward_pre_hook"><strong>register_full_backward_pre_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;for&nbsp;the&nbsp;module&nbsp;are&nbsp;computed.<br>
The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple[Tensor]&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_output`&nbsp;is&nbsp;a&nbsp;tuple.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;output&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_output`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;Entries&nbsp;in&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for<br>
all&nbsp;non-Tensor&nbsp;arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_load_state_dict_post_hook"><strong>register_load_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;after&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;incompatible_keys)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;``module``&nbsp;argument&nbsp;is&nbsp;the&nbsp;current&nbsp;module&nbsp;that&nbsp;this&nbsp;hook&nbsp;is&nbsp;registered<br>
on,&nbsp;and&nbsp;the&nbsp;``incompatible_keys``&nbsp;argument&nbsp;is&nbsp;a&nbsp;``NamedTuple``&nbsp;consisting<br>
of&nbsp;attributes&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``.&nbsp;``missing_keys``<br>
is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys&nbsp;and<br>
``unexpected_keys``&nbsp;is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys.<br>
&nbsp;<br>
The&nbsp;given&nbsp;incompatible_keys&nbsp;can&nbsp;be&nbsp;modified&nbsp;inplace&nbsp;if&nbsp;needed.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;checks&nbsp;performed&nbsp;when&nbsp;calling&nbsp;:func:`load_state_dict`&nbsp;with<br>
``strict=True``&nbsp;are&nbsp;affected&nbsp;by&nbsp;modifications&nbsp;the&nbsp;hook&nbsp;makes&nbsp;to<br>
``missing_keys``&nbsp;or&nbsp;``unexpected_keys``,&nbsp;as&nbsp;expected.&nbsp;Additions&nbsp;to&nbsp;either<br>
set&nbsp;of&nbsp;keys&nbsp;will&nbsp;result&nbsp;in&nbsp;an&nbsp;error&nbsp;being&nbsp;thrown&nbsp;when&nbsp;``strict=True``,&nbsp;and<br>
clearing&nbsp;out&nbsp;both&nbsp;missing&nbsp;and&nbsp;unexpected&nbsp;keys&nbsp;will&nbsp;avoid&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_load_state_dict_pre_hook"><strong>register_load_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;before&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata,&nbsp;strict,&nbsp;missing_keys,&nbsp;unexpected_keys,&nbsp;error_msgs)&nbsp;-&gt;&nbsp;None&nbsp;&nbsp;#&nbsp;noqa:&nbsp;B950<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;Callable&nbsp;hook&nbsp;that&nbsp;will&nbsp;be&nbsp;invoked&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loading&nbsp;the&nbsp;state&nbsp;dict.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_module"><strong>register_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Alias&nbsp;for&nbsp;:func:`add_module`.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_parameter"><strong>register_parameter</strong></a>(
    self,
    name: str,
    param: Optional[torch.nn.parameter.Parameter]
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter&nbsp;or&nbsp;None):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``None``,&nbsp;then&nbsp;operations&nbsp;that&nbsp;run&nbsp;on&nbsp;parameters,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,&nbsp;the&nbsp;parameter&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;module's&nbsp;:attr:`state_dict`.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_state_dict_post_hook"><strong>register_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;modify&nbsp;the&nbsp;``state_dict``&nbsp;inplace.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-register_state_dict_pre_hook"><strong>register_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;prefix,&nbsp;keep_vars)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;perform&nbsp;pre-processing&nbsp;before&nbsp;the&nbsp;``state_dict``<br>
call&nbsp;is&nbsp;made.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-requires_grad_"><strong>requires_grad_</strong></a>(self: ~T, requires_grad: bool = True) -&gt; ~T</dt><dd><span class="code">Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#AttentionBasedEstimator-requires_grad_">requires_grad_</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-set_extra_state"><strong>set_extra_state</strong></a>(self, state: Any) -&gt; None</dt><dd><span class="code">Set&nbsp;extra&nbsp;state&nbsp;contained&nbsp;in&nbsp;the&nbsp;loaded&nbsp;`state_dict`.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;called&nbsp;from&nbsp;:func:`load_state_dict`&nbsp;to&nbsp;handle&nbsp;any&nbsp;extra&nbsp;state<br>
found&nbsp;within&nbsp;the&nbsp;`state_dict`.&nbsp;Implement&nbsp;this&nbsp;function&nbsp;and&nbsp;a&nbsp;corresponding<br>
:func:`get_extra_state`&nbsp;for&nbsp;your&nbsp;module&nbsp;if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state&nbsp;within&nbsp;its<br>
`state_dict`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;(dict):&nbsp;Extra&nbsp;state&nbsp;from&nbsp;the&nbsp;`state_dict`</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-set_submodule"><strong>set_submodule</strong></a>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</dt><dd><span class="code">Set&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``False``&nbsp;(default),&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error&nbsp;if<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;submodule&nbsp;does&nbsp;not&nbsp;exist.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(3,&nbsp;3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;override&nbsp;the&nbsp;``Conv2d``&nbsp;with&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Linear``,&nbsp;you<br>
could&nbsp;call&nbsp;``<a href="#AttentionBasedEstimator-set_submodule">set_submodule</a>("net_b.net_c.conv",&nbsp;nn.Linear(1,&nbsp;1))``<br>
where&nbsp;``strict``&nbsp;could&nbsp;be&nbsp;``True``&nbsp;or&nbsp;``False``<br>
&nbsp;<br>
To&nbsp;add&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Conv2d``&nbsp;to&nbsp;the&nbsp;existing&nbsp;``net_b``&nbsp;module,<br>
you&nbsp;would&nbsp;call&nbsp;``<a href="#AttentionBasedEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1))``.<br>
&nbsp;<br>
In&nbsp;the&nbsp;above&nbsp;if&nbsp;you&nbsp;set&nbsp;``strict=True``&nbsp;and&nbsp;call<br>
``<a href="#AttentionBasedEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1),&nbsp;strict=True)``,&nbsp;an&nbsp;AttributeError<br>
will&nbsp;be&nbsp;raised&nbsp;because&nbsp;``net_b``&nbsp;does&nbsp;not&nbsp;have&nbsp;a&nbsp;submodule&nbsp;named&nbsp;``conv``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;module:&nbsp;The&nbsp;module&nbsp;to&nbsp;set&nbsp;the&nbsp;submodule&nbsp;to.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;If&nbsp;``False``,&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;the&nbsp;submodule&nbsp;doesn't&nbsp;already&nbsp;exist.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;``target``&nbsp;string&nbsp;is&nbsp;empty&nbsp;or&nbsp;if&nbsp;``module``&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;``target``&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-share_memory"><strong>share_memory</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">See&nbsp;:meth:`torch.Tensor.share_memory_`.</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-state_dict"><strong>state_dict</strong></a>(self, *args, destination=None, prefix='', keep_vars=False)</dt><dd><span class="code">Return&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;references&nbsp;to&nbsp;the&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
Parameters&nbsp;and&nbsp;buffers&nbsp;set&nbsp;to&nbsp;``None``&nbsp;are&nbsp;not&nbsp;included.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;returned&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;a&nbsp;shallow&nbsp;copy.&nbsp;It&nbsp;contains&nbsp;references<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;module's&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;``<a href="#AttentionBasedEstimator-state_dict">state_dict</a>()``&nbsp;also&nbsp;accepts&nbsp;positional&nbsp;arguments&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;``destination``,&nbsp;``prefix``&nbsp;and&nbsp;``keep_vars``&nbsp;in&nbsp;order.&nbsp;However,<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;being&nbsp;deprecated&nbsp;and&nbsp;keyword&nbsp;arguments&nbsp;will&nbsp;be&nbsp;enforced&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;future&nbsp;releases.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;avoid&nbsp;the&nbsp;use&nbsp;of&nbsp;argument&nbsp;``destination``&nbsp;as&nbsp;it&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;designed&nbsp;for&nbsp;end-users.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;destination&nbsp;(dict,&nbsp;optional):&nbsp;If&nbsp;provided,&nbsp;the&nbsp;state&nbsp;of&nbsp;module&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;updated&nbsp;into&nbsp;the&nbsp;dict&nbsp;and&nbsp;the&nbsp;same&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;an&nbsp;``OrderedDict``&nbsp;will&nbsp;be&nbsp;created&nbsp;and&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``None``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str,&nbsp;optional):&nbsp;a&nbsp;prefix&nbsp;added&nbsp;to&nbsp;parameter&nbsp;and&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;names&nbsp;to&nbsp;compose&nbsp;the&nbsp;keys&nbsp;in&nbsp;state_dict.&nbsp;Default:&nbsp;``''``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;keep_vars&nbsp;(bool,&nbsp;optional):&nbsp;by&nbsp;default&nbsp;the&nbsp;:class:`~torch.Tensor`&nbsp;s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returned&nbsp;in&nbsp;the&nbsp;state&nbsp;dict&nbsp;are&nbsp;detached&nbsp;from&nbsp;autograd.&nbsp;If&nbsp;it's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;to&nbsp;``True``,&nbsp;detaching&nbsp;will&nbsp;not&nbsp;be&nbsp;performed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#AttentionBasedEstimator-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Move&nbsp;and/or&nbsp;cast&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#AttentionBasedEstimator-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#AttentionBasedEstimator-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#AttentionBasedEstimator-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#AttentionBasedEstimator-to">to</a>(memory_format=torch.channels_last)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;:attr:`dtype`\&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;dtype&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_format&nbsp;(:class:`torch.memory_format`):&nbsp;the&nbsp;desired&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format&nbsp;for&nbsp;4D&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module&nbsp;(keyword<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;argument)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+IGNORE_WANT("non-deterministic")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#AttentionBasedEstimator-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+REQUIRES(env:TORCH_DOCTEST_CUDA1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#AttentionBasedEstimator-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#AttentionBasedEstimator-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2,&nbsp;bias=None).<a href="#AttentionBasedEstimator-to">to</a>(torch.cdouble)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.3741+0.j,&nbsp;&nbsp;0.2382+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;0.5593+0.j,&nbsp;-0.4443+0.j]],&nbsp;dtype=torch.complex128)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear(torch.ones(3,&nbsp;2,&nbsp;dtype=torch.cdouble))<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j]],&nbsp;dtype=torch.complex128)</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-to_empty"><strong>to_empty</strong></a>(
    self: ~T,
    *,
    device: Union[int, str, torch.device, NoneType],
    recurse: bool = True
) -&gt; ~T</dt><dd><span class="code">Move&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;specified&nbsp;device&nbsp;without&nbsp;copying&nbsp;storage.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;The&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;Whether&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;of&nbsp;submodules&nbsp;should<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;recursively&nbsp;moved&nbsp;to&nbsp;the&nbsp;specified&nbsp;device.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-train"><strong>train</strong></a>(self: ~T, mode: bool = True) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.,&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-type"><strong>type</strong></a>(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-xpu"><strong>xpu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;XPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;XPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="AttentionBasedEstimator-zero_grad"><strong>zero_grad</strong></a>(self, set_to_none: bool = True) -&gt; None</dt><dd><span class="code">Reset&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters.<br>
&nbsp;<br>
See&nbsp;similar&nbsp;function&nbsp;under&nbsp;:class:`torch.optim.Optimizer`&nbsp;for&nbsp;more&nbsp;context.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;set_to_none&nbsp;(bool):&nbsp;instead&nbsp;of&nbsp;setting&nbsp;to&nbsp;zero,&nbsp;set&nbsp;the&nbsp;grads&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;:meth:`torch.optim.Optimizer.zero_grad`&nbsp;for&nbsp;details.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>T_destination</strong> = ~T_destination</dl>

<dl><dt><strong>call_super_init</strong> = False</dl>

<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="ConfidenceEstimator">class <strong>ConfidenceEstimator</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#ConfidenceEstimator">ConfidenceEstimator</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;vocab_size:&nbsp;int&nbsp;=&nbsp;32000,<br>
&nbsp;&nbsp;&nbsp;&nbsp;hidden_dim:&nbsp;int&nbsp;=&nbsp;4096,<br>
&nbsp;&nbsp;&nbsp;&nbsp;symbolic_dim:&nbsp;int&nbsp;=&nbsp;512,<br>
&nbsp;&nbsp;&nbsp;&nbsp;methods:&nbsp;Optional[List[adapters.confidence_estimator.<a href="#ConfidenceMethod">ConfidenceMethod</a>]]&nbsp;=&nbsp;None<br>
)<br>
&nbsp;<br>
Comprehensive&nbsp;Confidence&nbsp;Estimator<br>
&nbsp;<br>
Combines&nbsp;multiple&nbsp;uncertainty&nbsp;quantification&nbsp;methods&nbsp;for&nbsp;robust<br>
confidence&nbsp;estimation&nbsp;in&nbsp;neurosymbolic&nbsp;SQL&nbsp;generation.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="adapters.confidence_estimator.html#ConfidenceEstimator">ConfidenceEstimator</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="ConfidenceEstimator-__init__"><strong>__init__</strong></a>(
    self,
    vocab_size: int = 32000,
    hidden_dim: int = 4096,
    symbolic_dim: int = 512,
    methods: Optional[List[adapters.confidence_estimator.ConfidenceMethod]] = None
)</dt><dd><span class="code">Initialize&nbsp;confidence&nbsp;estimator<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;vocab_size:&nbsp;Vocabulary&nbsp;size&nbsp;of&nbsp;the&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;hidden_dim:&nbsp;Hidden&nbsp;dimension&nbsp;of&nbsp;the&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;symbolic_dim:&nbsp;Dimension&nbsp;of&nbsp;symbolic&nbsp;representations<br>
&nbsp;&nbsp;&nbsp;&nbsp;methods:&nbsp;List&nbsp;of&nbsp;confidence&nbsp;estimation&nbsp;methods&nbsp;to&nbsp;use</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-analyze_uncertainty_sources"><strong>analyze_uncertainty_sources</strong></a>(
    self,
    logits: torch.Tensor,
    hidden_states: torch.Tensor,
    **kwargs
) -&gt; Dict[str, Any]</dt><dd><span class="code">Analyze&nbsp;different&nbsp;sources&nbsp;of&nbsp;uncertainty</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-calibrate_temperature"><strong>calibrate_temperature</strong></a>(
    self,
    validation_logits: torch.Tensor,
    validation_labels: torch.Tensor
) -&gt; None</dt><dd><span class="code">Calibrate&nbsp;temperature&nbsp;scaling&nbsp;using&nbsp;validation&nbsp;data</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-forward"><strong>forward</strong></a>(
    self,
    logits: torch.Tensor,
    hidden_states: torch.Tensor,
    attention_weights: Optional[torch.Tensor] = None,
    symbolic_embeddings: Optional[torch.Tensor] = None,
    extracted_facts: Optional[List[List[str]]] = None
) -&gt; adapters.confidence_estimator.ConfidenceOutput</dt><dd><span class="code">Estimate&nbsp;confidence&nbsp;using&nbsp;multiple&nbsp;methods<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits:&nbsp;Model&nbsp;logits&nbsp;[batch_size,&nbsp;seq_len,&nbsp;vocab_size]<br>
&nbsp;&nbsp;&nbsp;&nbsp;hidden_states:&nbsp;Hidden&nbsp;states&nbsp;[batch_size,&nbsp;seq_len,&nbsp;hidden_dim]<br>
&nbsp;&nbsp;&nbsp;&nbsp;attention_weights:&nbsp;Attention&nbsp;weights&nbsp;(optional)<br>
&nbsp;&nbsp;&nbsp;&nbsp;symbolic_embeddings:&nbsp;Symbolic&nbsp;representations&nbsp;(optional)<br>
&nbsp;&nbsp;&nbsp;&nbsp;extracted_facts:&nbsp;Extracted&nbsp;facts&nbsp;(optional)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#ConfidenceOutput">ConfidenceOutput</a>&nbsp;with&nbsp;comprehensive&nbsp;confidence&nbsp;estimates</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-get_method_weights"><strong>get_method_weights</strong></a>(self) -&gt; Dict[str, float]</dt><dd><span class="code">Get&nbsp;learned&nbsp;weights&nbsp;for&nbsp;different&nbsp;confidence&nbsp;methods</span></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__annotations__</strong> = {}</dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="ConfidenceEstimator-__call__"><strong>__call__</strong></a> = <a href="#ConfidenceEstimator-_wrapped_call_impl">_wrapped_call_impl</a>(self, *args, **kwargs)</dt></dl>

<dl><dt><a name="ConfidenceEstimator-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><span class="code">Implement&nbsp;delattr(self,&nbsp;name).</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><span class="code">Default&nbsp;dir()&nbsp;implementation.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-__getattr__"><strong>__getattr__</strong></a>(self, name: str) -&gt; Union[torch.Tensor, ForwardRef('Module')]</dt><dd><span class="code">#&nbsp;It&nbsp;is&nbsp;crucial&nbsp;that&nbsp;the&nbsp;return&nbsp;type&nbsp;is&nbsp;not&nbsp;annotated&nbsp;as&nbsp;`Any`,&nbsp;otherwise&nbsp;type&nbsp;checking<br>
#&nbsp;on&nbsp;`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;and&nbsp;all&nbsp;its&nbsp;subclasses&nbsp;is&nbsp;largely&nbsp;disabled&nbsp;as&nbsp;a&nbsp;result.&nbsp;See:<br>
#&nbsp;<a href="https://github.com/pytorch/pytorch/pull/115074">https://github.com/pytorch/pytorch/pull/115074</a></span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-__getstate__"><strong>__getstate__</strong></a>(self)</dt><dd><span class="code">Helper&nbsp;for&nbsp;pickle.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-__setattr__"><strong>__setattr__</strong></a>(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="ConfidenceEstimator-add_module"><strong>add_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-apply"><strong>apply</strong></a>(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</dt><dd><span class="code">Apply&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#ConfidenceEstimator-children">children</a>()``)&nbsp;as&nbsp;well&nbsp;as&nbsp;self.<br>
&nbsp;<br>
Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;@torch.no_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#ConfidenceEstimator-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#ConfidenceEstimator-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-bfloat16"><strong>bfloat16</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``bfloat16``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-buffers"><strong>buffers</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#ConfidenceEstimator-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#ConfidenceEstimator-type">type</a>(buf),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-children"><strong>children</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-compile"><strong>compile</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Compile&nbsp;this&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;using&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
This&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;`__call__`&nbsp;method&nbsp;is&nbsp;compiled&nbsp;and&nbsp;all&nbsp;arguments&nbsp;are&nbsp;passed&nbsp;as-is<br>
to&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
See&nbsp;:func:`torch.compile`&nbsp;for&nbsp;details&nbsp;on&nbsp;the&nbsp;arguments&nbsp;for&nbsp;this&nbsp;function.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-cpu"><strong>cpu</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-cuda"><strong>cuda</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-double"><strong>double</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-eval"><strong>eval</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#ConfidenceEstimator-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#ConfidenceEstimator-eval">eval</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-extra_repr"><strong>extra_repr</strong></a>(self) -&gt; str</dt><dd><span class="code">Return&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;re-implement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-float"><strong>float</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``float``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-get_buffer"><strong>get_buffer</strong></a>(self, target: str) -&gt; 'Tensor'</dt><dd><span class="code">Return&nbsp;the&nbsp;buffer&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;The&nbsp;buffer&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-get_extra_state"><strong>get_extra_state</strong></a>(self) -&gt; Any</dt><dd><span class="code">Return&nbsp;any&nbsp;extra&nbsp;state&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict.<br>
&nbsp;<br>
Implement&nbsp;this&nbsp;and&nbsp;a&nbsp;corresponding&nbsp;:func:`set_extra_state`&nbsp;for&nbsp;your&nbsp;module<br>
if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state.&nbsp;This&nbsp;function&nbsp;is&nbsp;called&nbsp;when&nbsp;building&nbsp;the<br>
module's&nbsp;`<a href="#ConfidenceEstimator-state_dict">state_dict</a>()`.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;extra&nbsp;state&nbsp;should&nbsp;be&nbsp;picklable&nbsp;to&nbsp;ensure&nbsp;working&nbsp;serialization<br>
of&nbsp;the&nbsp;state_dict.&nbsp;We&nbsp;only&nbsp;provide&nbsp;backwards&nbsp;compatibility&nbsp;guarantees<br>
for&nbsp;serializing&nbsp;Tensors;&nbsp;other&nbsp;objects&nbsp;may&nbsp;break&nbsp;backwards&nbsp;compatibility&nbsp;if<br>
their&nbsp;serialized&nbsp;pickled&nbsp;form&nbsp;changes.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="builtins.html#object">object</a>:&nbsp;Any&nbsp;extra&nbsp;state&nbsp;to&nbsp;store&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-get_parameter"><strong>get_parameter</strong></a>(self, target: str) -&gt; 'Parameter'</dt><dd><span class="code">Return&nbsp;the&nbsp;parameter&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;Parameter<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Parameter:&nbsp;The&nbsp;Parameter&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``nn.Parameter``</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-get_submodule"><strong>get_submodule</strong></a>(self, target: str) -&gt; 'Module'</dt><dd><span class="code">Return&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(16,&nbsp;33,&nbsp;kernel_size=(3,&nbsp;3),&nbsp;stride=(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(in_features=100,&nbsp;out_features=200,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;which&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;check&nbsp;whether&nbsp;or&nbsp;not&nbsp;we&nbsp;have&nbsp;the&nbsp;``linear``&nbsp;submodule,&nbsp;we<br>
would&nbsp;call&nbsp;``<a href="#ConfidenceEstimator-get_submodule">get_submodule</a>("net_b.linear")``.&nbsp;To&nbsp;check&nbsp;whether<br>
we&nbsp;have&nbsp;the&nbsp;``conv``&nbsp;submodule,&nbsp;we&nbsp;would&nbsp;call<br>
``<a href="#ConfidenceEstimator-get_submodule">get_submodule</a>("net_b.net_c.conv")``.<br>
&nbsp;<br>
The&nbsp;runtime&nbsp;of&nbsp;``get_submodule``&nbsp;is&nbsp;bounded&nbsp;by&nbsp;the&nbsp;degree<br>
of&nbsp;module&nbsp;nesting&nbsp;in&nbsp;``target``.&nbsp;A&nbsp;query&nbsp;against<br>
``named_modules``&nbsp;achieves&nbsp;the&nbsp;same&nbsp;result,&nbsp;but&nbsp;it&nbsp;is&nbsp;O(N)&nbsp;in<br>
the&nbsp;number&nbsp;of&nbsp;transitive&nbsp;modules.&nbsp;So,&nbsp;for&nbsp;a&nbsp;simple&nbsp;check&nbsp;to&nbsp;see<br>
if&nbsp;some&nbsp;submodule&nbsp;exists,&nbsp;``get_submodule``&nbsp;should&nbsp;always&nbsp;be<br>
used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;The&nbsp;submodule&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-half"><strong>half</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-ipu"><strong>ipu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;IPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;IPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-load_state_dict"><strong>load_state_dict</strong></a>(
    self,
    state_dict: collections.abc.Mapping[str, typing.Any],
    strict: bool = True,
    assign: bool = False
)</dt><dd><span class="code">Copy&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into&nbsp;this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.<br>
&nbsp;<br>
If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;:attr:`assign`&nbsp;is&nbsp;``True``&nbsp;the&nbsp;optimizer&nbsp;must&nbsp;be&nbsp;created&nbsp;after<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;call&nbsp;to&nbsp;:attr:`load_state_dict`&nbsp;unless<br>
&nbsp;&nbsp;&nbsp;&nbsp;:func:`~torch.__future__.get_swap_module_params_on_conversion`&nbsp;is&nbsp;``True``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;&nbsp;&nbsp;&nbsp;assign&nbsp;(bool,&nbsp;optional):&nbsp;When&nbsp;set&nbsp;to&nbsp;``False``,&nbsp;the&nbsp;properties&nbsp;of&nbsp;the&nbsp;tensors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;current&nbsp;module&nbsp;are&nbsp;preserved&nbsp;whereas&nbsp;setting&nbsp;it&nbsp;to&nbsp;``True``&nbsp;preserves<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;properties&nbsp;of&nbsp;the&nbsp;Tensors&nbsp;in&nbsp;the&nbsp;state&nbsp;dict.&nbsp;The&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exception&nbsp;is&nbsp;the&nbsp;``requires_grad``&nbsp;field&nbsp;of&nbsp;:class:`~torch.nn.Parameter`s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;which&nbsp;the&nbsp;value&nbsp;from&nbsp;the&nbsp;module&nbsp;is&nbsp;preserved.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;any&nbsp;keys&nbsp;that&nbsp;are&nbsp;expected<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;missing&nbsp;from&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;keys&nbsp;that&nbsp;are&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expected&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;present&nbsp;in&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;a&nbsp;parameter&nbsp;or&nbsp;buffer&nbsp;is&nbsp;registered&nbsp;as&nbsp;``None``&nbsp;and&nbsp;its&nbsp;corresponding&nbsp;key<br>
&nbsp;&nbsp;&nbsp;&nbsp;exists&nbsp;in&nbsp;:attr:`state_dict`,&nbsp;:meth:`load_state_dict`&nbsp;will&nbsp;raise&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;``RuntimeError``.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-modules"><strong>modules</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#ConfidenceEstimator-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-mtia"><strong>mtia</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;MTIA.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;MTIA&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-named_buffers"><strong>named_buffers</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool,&nbsp;optional):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;buffers&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#ConfidenceEstimator-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-named_children"><strong>named_children</strong></a>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#ConfidenceEstimator-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-named_modules"><strong>named_modules</strong></a>(
    self,
    memo: Optional[set['Module']] = None,
    prefix: str = '',
    remove_duplicate: bool = True
)</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;memo:&nbsp;a&nbsp;memo&nbsp;to&nbsp;store&nbsp;the&nbsp;set&nbsp;of&nbsp;modules&nbsp;already&nbsp;added&nbsp;to&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix:&nbsp;a&nbsp;prefix&nbsp;that&nbsp;will&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate:&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;module&nbsp;instances&nbsp;in&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;not<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#ConfidenceEstimator-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-named_parameters"><strong>named_parameters</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#ConfidenceEstimator-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-parameters"><strong>parameters</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#ConfidenceEstimator-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#ConfidenceEstimator-type">type</a>(param),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_backward_hook"><strong>register_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;deprecated&nbsp;in&nbsp;favor&nbsp;of&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.register_full_backward_hook`&nbsp;and<br>
the&nbsp;behavior&nbsp;of&nbsp;this&nbsp;function&nbsp;will&nbsp;change&nbsp;in&nbsp;future&nbsp;versions.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_buffer"><strong>register_buffer</strong></a>(
    self,
    name: str,
    tensor: Optional[torch.Tensor],
    persistent: bool = True
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;module's&nbsp;state.&nbsp;Buffers,&nbsp;by<br>
default,&nbsp;are&nbsp;persistent&nbsp;and&nbsp;will&nbsp;be&nbsp;saved&nbsp;alongside&nbsp;parameters.&nbsp;This<br>
behavior&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;setting&nbsp;:attr:`persistent`&nbsp;to&nbsp;``False``.&nbsp;The<br>
only&nbsp;difference&nbsp;between&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;and&nbsp;a&nbsp;non-persistent&nbsp;buffer<br>
is&nbsp;that&nbsp;the&nbsp;latter&nbsp;will&nbsp;not&nbsp;be&nbsp;a&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
:attr:`state_dict`.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor&nbsp;or&nbsp;None):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.&nbsp;If&nbsp;``None``,&nbsp;then&nbsp;operations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;run&nbsp;on&nbsp;buffers,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;buffer&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the&nbsp;module's&nbsp;:attr:`state_dict`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;(bool):&nbsp;whether&nbsp;the&nbsp;buffer&nbsp;is&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:attr:`state_dict`.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#ConfidenceEstimator-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_forward_hook"><strong>register_forward_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False,
    always_call: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``False``&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but&nbsp;it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on<br>
forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after&nbsp;:func:`forward`&nbsp;is&nbsp;called.&nbsp;The&nbsp;hook<br>
should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``True``,&nbsp;the&nbsp;forward&nbsp;hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
``kwargs``&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function&nbsp;and&nbsp;be&nbsp;expected&nbsp;to&nbsp;return&nbsp;the<br>
output&nbsp;possibly&nbsp;modified.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;always_call&nbsp;(bool):&nbsp;If&nbsp;``True``&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;run&nbsp;regardless&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;whether&nbsp;an&nbsp;exception&nbsp;is&nbsp;raised&nbsp;while&nbsp;calling&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
&nbsp;<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;false&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a&nbsp;single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the<br>
hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple&nbsp;if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned<br>
(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the<br>
following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;true,&nbsp;the&nbsp;forward&nbsp;pre-hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.&nbsp;And&nbsp;if&nbsp;the&nbsp;hook&nbsp;modifies&nbsp;the<br>
input,&nbsp;both&nbsp;the&nbsp;args&nbsp;and&nbsp;kwargs&nbsp;should&nbsp;be&nbsp;returned.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have<br>
the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;a&nbsp;tuple&nbsp;of&nbsp;modified&nbsp;input&nbsp;and&nbsp;kwargs<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the&nbsp;kwargs<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_full_backward_hook"><strong>register_full_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;a&nbsp;module<br>
are&nbsp;computed,&nbsp;i.e.&nbsp;the&nbsp;hook&nbsp;will&nbsp;execute&nbsp;if&nbsp;and&nbsp;only&nbsp;if&nbsp;the&nbsp;gradients&nbsp;with<br>
respect&nbsp;to&nbsp;module&nbsp;outputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following<br>
signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple(Tensor)&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;are&nbsp;tuples&nbsp;that&nbsp;contain&nbsp;the&nbsp;gradients<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs&nbsp;respectively.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;:attr:`grad_input`&nbsp;will&nbsp;only&nbsp;correspond&nbsp;to&nbsp;the&nbsp;inputs&nbsp;given<br>
as&nbsp;positional&nbsp;arguments&nbsp;and&nbsp;all&nbsp;kwarg&nbsp;arguments&nbsp;are&nbsp;ignored.&nbsp;Entries<br>
in&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for&nbsp;all&nbsp;non-Tensor<br>
arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;or&nbsp;outputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_full_backward_pre_hook"><strong>register_full_backward_pre_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;for&nbsp;the&nbsp;module&nbsp;are&nbsp;computed.<br>
The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple[Tensor]&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_output`&nbsp;is&nbsp;a&nbsp;tuple.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;output&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_output`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;Entries&nbsp;in&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for<br>
all&nbsp;non-Tensor&nbsp;arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_load_state_dict_post_hook"><strong>register_load_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;after&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;incompatible_keys)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;``module``&nbsp;argument&nbsp;is&nbsp;the&nbsp;current&nbsp;module&nbsp;that&nbsp;this&nbsp;hook&nbsp;is&nbsp;registered<br>
on,&nbsp;and&nbsp;the&nbsp;``incompatible_keys``&nbsp;argument&nbsp;is&nbsp;a&nbsp;``NamedTuple``&nbsp;consisting<br>
of&nbsp;attributes&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``.&nbsp;``missing_keys``<br>
is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys&nbsp;and<br>
``unexpected_keys``&nbsp;is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys.<br>
&nbsp;<br>
The&nbsp;given&nbsp;incompatible_keys&nbsp;can&nbsp;be&nbsp;modified&nbsp;inplace&nbsp;if&nbsp;needed.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;checks&nbsp;performed&nbsp;when&nbsp;calling&nbsp;:func:`load_state_dict`&nbsp;with<br>
``strict=True``&nbsp;are&nbsp;affected&nbsp;by&nbsp;modifications&nbsp;the&nbsp;hook&nbsp;makes&nbsp;to<br>
``missing_keys``&nbsp;or&nbsp;``unexpected_keys``,&nbsp;as&nbsp;expected.&nbsp;Additions&nbsp;to&nbsp;either<br>
set&nbsp;of&nbsp;keys&nbsp;will&nbsp;result&nbsp;in&nbsp;an&nbsp;error&nbsp;being&nbsp;thrown&nbsp;when&nbsp;``strict=True``,&nbsp;and<br>
clearing&nbsp;out&nbsp;both&nbsp;missing&nbsp;and&nbsp;unexpected&nbsp;keys&nbsp;will&nbsp;avoid&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_load_state_dict_pre_hook"><strong>register_load_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;before&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata,&nbsp;strict,&nbsp;missing_keys,&nbsp;unexpected_keys,&nbsp;error_msgs)&nbsp;-&gt;&nbsp;None&nbsp;&nbsp;#&nbsp;noqa:&nbsp;B950<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;Callable&nbsp;hook&nbsp;that&nbsp;will&nbsp;be&nbsp;invoked&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loading&nbsp;the&nbsp;state&nbsp;dict.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_module"><strong>register_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Alias&nbsp;for&nbsp;:func:`add_module`.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_parameter"><strong>register_parameter</strong></a>(
    self,
    name: str,
    param: Optional[torch.nn.parameter.Parameter]
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter&nbsp;or&nbsp;None):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``None``,&nbsp;then&nbsp;operations&nbsp;that&nbsp;run&nbsp;on&nbsp;parameters,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,&nbsp;the&nbsp;parameter&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;module's&nbsp;:attr:`state_dict`.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_state_dict_post_hook"><strong>register_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;modify&nbsp;the&nbsp;``state_dict``&nbsp;inplace.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-register_state_dict_pre_hook"><strong>register_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;prefix,&nbsp;keep_vars)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;perform&nbsp;pre-processing&nbsp;before&nbsp;the&nbsp;``state_dict``<br>
call&nbsp;is&nbsp;made.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-requires_grad_"><strong>requires_grad_</strong></a>(self: ~T, requires_grad: bool = True) -&gt; ~T</dt><dd><span class="code">Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#ConfidenceEstimator-requires_grad_">requires_grad_</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-set_extra_state"><strong>set_extra_state</strong></a>(self, state: Any) -&gt; None</dt><dd><span class="code">Set&nbsp;extra&nbsp;state&nbsp;contained&nbsp;in&nbsp;the&nbsp;loaded&nbsp;`state_dict`.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;called&nbsp;from&nbsp;:func:`load_state_dict`&nbsp;to&nbsp;handle&nbsp;any&nbsp;extra&nbsp;state<br>
found&nbsp;within&nbsp;the&nbsp;`state_dict`.&nbsp;Implement&nbsp;this&nbsp;function&nbsp;and&nbsp;a&nbsp;corresponding<br>
:func:`get_extra_state`&nbsp;for&nbsp;your&nbsp;module&nbsp;if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state&nbsp;within&nbsp;its<br>
`state_dict`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;(dict):&nbsp;Extra&nbsp;state&nbsp;from&nbsp;the&nbsp;`state_dict`</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-set_submodule"><strong>set_submodule</strong></a>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</dt><dd><span class="code">Set&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``False``&nbsp;(default),&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error&nbsp;if<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;submodule&nbsp;does&nbsp;not&nbsp;exist.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(3,&nbsp;3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;override&nbsp;the&nbsp;``Conv2d``&nbsp;with&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Linear``,&nbsp;you<br>
could&nbsp;call&nbsp;``<a href="#ConfidenceEstimator-set_submodule">set_submodule</a>("net_b.net_c.conv",&nbsp;nn.Linear(1,&nbsp;1))``<br>
where&nbsp;``strict``&nbsp;could&nbsp;be&nbsp;``True``&nbsp;or&nbsp;``False``<br>
&nbsp;<br>
To&nbsp;add&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Conv2d``&nbsp;to&nbsp;the&nbsp;existing&nbsp;``net_b``&nbsp;module,<br>
you&nbsp;would&nbsp;call&nbsp;``<a href="#ConfidenceEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1))``.<br>
&nbsp;<br>
In&nbsp;the&nbsp;above&nbsp;if&nbsp;you&nbsp;set&nbsp;``strict=True``&nbsp;and&nbsp;call<br>
``<a href="#ConfidenceEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1),&nbsp;strict=True)``,&nbsp;an&nbsp;AttributeError<br>
will&nbsp;be&nbsp;raised&nbsp;because&nbsp;``net_b``&nbsp;does&nbsp;not&nbsp;have&nbsp;a&nbsp;submodule&nbsp;named&nbsp;``conv``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;module:&nbsp;The&nbsp;module&nbsp;to&nbsp;set&nbsp;the&nbsp;submodule&nbsp;to.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;If&nbsp;``False``,&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;the&nbsp;submodule&nbsp;doesn't&nbsp;already&nbsp;exist.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;``target``&nbsp;string&nbsp;is&nbsp;empty&nbsp;or&nbsp;if&nbsp;``module``&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;``target``&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-share_memory"><strong>share_memory</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">See&nbsp;:meth:`torch.Tensor.share_memory_`.</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-state_dict"><strong>state_dict</strong></a>(self, *args, destination=None, prefix='', keep_vars=False)</dt><dd><span class="code">Return&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;references&nbsp;to&nbsp;the&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
Parameters&nbsp;and&nbsp;buffers&nbsp;set&nbsp;to&nbsp;``None``&nbsp;are&nbsp;not&nbsp;included.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;returned&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;a&nbsp;shallow&nbsp;copy.&nbsp;It&nbsp;contains&nbsp;references<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;module's&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;``<a href="#ConfidenceEstimator-state_dict">state_dict</a>()``&nbsp;also&nbsp;accepts&nbsp;positional&nbsp;arguments&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;``destination``,&nbsp;``prefix``&nbsp;and&nbsp;``keep_vars``&nbsp;in&nbsp;order.&nbsp;However,<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;being&nbsp;deprecated&nbsp;and&nbsp;keyword&nbsp;arguments&nbsp;will&nbsp;be&nbsp;enforced&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;future&nbsp;releases.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;avoid&nbsp;the&nbsp;use&nbsp;of&nbsp;argument&nbsp;``destination``&nbsp;as&nbsp;it&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;designed&nbsp;for&nbsp;end-users.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;destination&nbsp;(dict,&nbsp;optional):&nbsp;If&nbsp;provided,&nbsp;the&nbsp;state&nbsp;of&nbsp;module&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;updated&nbsp;into&nbsp;the&nbsp;dict&nbsp;and&nbsp;the&nbsp;same&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;an&nbsp;``OrderedDict``&nbsp;will&nbsp;be&nbsp;created&nbsp;and&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``None``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str,&nbsp;optional):&nbsp;a&nbsp;prefix&nbsp;added&nbsp;to&nbsp;parameter&nbsp;and&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;names&nbsp;to&nbsp;compose&nbsp;the&nbsp;keys&nbsp;in&nbsp;state_dict.&nbsp;Default:&nbsp;``''``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;keep_vars&nbsp;(bool,&nbsp;optional):&nbsp;by&nbsp;default&nbsp;the&nbsp;:class:`~torch.Tensor`&nbsp;s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returned&nbsp;in&nbsp;the&nbsp;state&nbsp;dict&nbsp;are&nbsp;detached&nbsp;from&nbsp;autograd.&nbsp;If&nbsp;it's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;to&nbsp;``True``,&nbsp;detaching&nbsp;will&nbsp;not&nbsp;be&nbsp;performed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#ConfidenceEstimator-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Move&nbsp;and/or&nbsp;cast&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#ConfidenceEstimator-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#ConfidenceEstimator-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#ConfidenceEstimator-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#ConfidenceEstimator-to">to</a>(memory_format=torch.channels_last)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;:attr:`dtype`\&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;dtype&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_format&nbsp;(:class:`torch.memory_format`):&nbsp;the&nbsp;desired&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format&nbsp;for&nbsp;4D&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module&nbsp;(keyword<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;argument)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+IGNORE_WANT("non-deterministic")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#ConfidenceEstimator-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+REQUIRES(env:TORCH_DOCTEST_CUDA1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#ConfidenceEstimator-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#ConfidenceEstimator-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2,&nbsp;bias=None).<a href="#ConfidenceEstimator-to">to</a>(torch.cdouble)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.3741+0.j,&nbsp;&nbsp;0.2382+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;0.5593+0.j,&nbsp;-0.4443+0.j]],&nbsp;dtype=torch.complex128)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear(torch.ones(3,&nbsp;2,&nbsp;dtype=torch.cdouble))<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j]],&nbsp;dtype=torch.complex128)</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-to_empty"><strong>to_empty</strong></a>(
    self: ~T,
    *,
    device: Union[int, str, torch.device, NoneType],
    recurse: bool = True
) -&gt; ~T</dt><dd><span class="code">Move&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;specified&nbsp;device&nbsp;without&nbsp;copying&nbsp;storage.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;The&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;Whether&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;of&nbsp;submodules&nbsp;should<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;recursively&nbsp;moved&nbsp;to&nbsp;the&nbsp;specified&nbsp;device.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-train"><strong>train</strong></a>(self: ~T, mode: bool = True) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.,&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-type"><strong>type</strong></a>(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-xpu"><strong>xpu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;XPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;XPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="ConfidenceEstimator-zero_grad"><strong>zero_grad</strong></a>(self, set_to_none: bool = True) -&gt; None</dt><dd><span class="code">Reset&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters.<br>
&nbsp;<br>
See&nbsp;similar&nbsp;function&nbsp;under&nbsp;:class:`torch.optim.Optimizer`&nbsp;for&nbsp;more&nbsp;context.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;set_to_none&nbsp;(bool):&nbsp;instead&nbsp;of&nbsp;setting&nbsp;to&nbsp;zero,&nbsp;set&nbsp;the&nbsp;grads&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;:meth:`torch.optim.Optimizer.zero_grad`&nbsp;for&nbsp;details.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>T_destination</strong> = ~T_destination</dl>

<dl><dt><strong>call_super_init</strong> = False</dl>

<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="ConfidenceMethod">class <strong>ConfidenceMethod</strong></a>(<a href="enum.html#Enum">enum.Enum</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#ConfidenceMethod">ConfidenceMethod</a>(*values)<br>
&nbsp;<br>
Available&nbsp;confidence&nbsp;estimation&nbsp;methods<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="adapters.confidence_estimator.html#ConfidenceMethod">ConfidenceMethod</a></dd>
<dd><a href="enum.html#Enum">enum.Enum</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>ATTENTION_BASED</strong> = &lt;ConfidenceMethod.ATTENTION_BASED: 'attention_based'&gt;</dl>

<dl><dt><strong>ENSEMBLE</strong> = &lt;ConfidenceMethod.ENSEMBLE: 'ensemble'&gt;</dl>

<dl><dt><strong>ENTROPY</strong> = &lt;ConfidenceMethod.ENTROPY: 'entropy'&gt;</dl>

<dl><dt><strong>MONTE_CARLO_DROPOUT</strong> = &lt;ConfidenceMethod.MONTE_CARLO_DROPOUT: 'monte_carlo_dropout'&gt;</dl>

<dl><dt><strong>SYMBOLIC_CONSISTENCY</strong> = &lt;ConfidenceMethod.SYMBOLIC_CONSISTENCY: 'symbolic_consistency'&gt;</dl>

<dl><dt><strong>TEMPERATURE_SCALING</strong> = &lt;ConfidenceMethod.TEMPERATURE_SCALING: 'temperature_scaling'&gt;</dl>

<hr>
Data descriptors inherited from <a href="enum.html#Enum">enum.Enum</a>:<br>
<dl><dt><strong>name</strong></dt>
<dd><span class="code">The&nbsp;name&nbsp;of&nbsp;the&nbsp;Enum&nbsp;member.</span></dd>
</dl>
<dl><dt><strong>value</strong></dt>
<dd><span class="code">The&nbsp;value&nbsp;of&nbsp;the&nbsp;Enum&nbsp;member.</span></dd>
</dl>
<hr>
Static methods inherited from <a href="enum.html#EnumType">enum.EnumType</a>:<br>
<dl><dt><a name="ConfidenceMethod-__contains__"><strong>__contains__</strong></a>(value)</dt><dd><span class="code">Return&nbsp;True&nbsp;if&nbsp;`value`&nbsp;is&nbsp;in&nbsp;`cls`.<br>
&nbsp;<br>
`value`&nbsp;is&nbsp;in&nbsp;`cls`&nbsp;if:<br>
1)&nbsp;`value`&nbsp;is&nbsp;a&nbsp;member&nbsp;of&nbsp;`cls`,&nbsp;or<br>
2)&nbsp;`value`&nbsp;is&nbsp;the&nbsp;value&nbsp;of&nbsp;one&nbsp;of&nbsp;the&nbsp;`cls`'s&nbsp;members.<br>
3)&nbsp;`value`&nbsp;is&nbsp;a&nbsp;pseudo-member&nbsp;(flags)</span></dd></dl>

<dl><dt><a name="ConfidenceMethod-__getitem__"><strong>__getitem__</strong></a>(name)</dt><dd><span class="code">Return&nbsp;the&nbsp;member&nbsp;matching&nbsp;`name`.</span></dd></dl>

<dl><dt><a name="ConfidenceMethod-__iter__"><strong>__iter__</strong></a>()</dt><dd><span class="code">Return&nbsp;members&nbsp;in&nbsp;definition&nbsp;order.</span></dd></dl>

<dl><dt><a name="ConfidenceMethod-__len__"><strong>__len__</strong></a>()</dt><dd><span class="code">Return&nbsp;the&nbsp;number&nbsp;of&nbsp;members&nbsp;(no&nbsp;aliases)</span></dd></dl>

<hr>
Readonly properties inherited from <a href="enum.html#EnumType">enum.EnumType</a>:<br>
<dl><dt><strong>__members__</strong></dt>
<dd><span class="code">Returns&nbsp;a&nbsp;mapping&nbsp;of&nbsp;member&nbsp;name-&gt;value.<br>
&nbsp;<br>
This&nbsp;mapping&nbsp;lists&nbsp;all&nbsp;enum&nbsp;members,&nbsp;including&nbsp;aliases.&nbsp;Note&nbsp;that&nbsp;this<br>
is&nbsp;a&nbsp;read-only&nbsp;view&nbsp;of&nbsp;the&nbsp;internal&nbsp;mapping.</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="ConfidenceOutput">class <strong>ConfidenceOutput</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#ConfidenceOutput">ConfidenceOutput</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;overall_confidence:&nbsp;float,<br>
&nbsp;&nbsp;&nbsp;&nbsp;token_confidences:&nbsp;Optional[torch.Tensor]&nbsp;=&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;uncertainty_estimate:&nbsp;Optional[float]&nbsp;=&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;calibration_score:&nbsp;Optional[float]&nbsp;=&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;method_scores:&nbsp;Optional[Dict[str,&nbsp;float]]&nbsp;=&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;explanation:&nbsp;Optional[str]&nbsp;=&nbsp;None<br>
)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
Output&nbsp;from&nbsp;confidence&nbsp;estimation<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="ConfidenceOutput-__eq__"><strong>__eq__</strong></a>(self, other)</dt><dd><span class="code">Return&nbsp;self==value.</span></dd></dl>

<dl><dt><a name="ConfidenceOutput-__init__"><strong>__init__</strong></a>(
    self,
    overall_confidence: float,
    token_confidences: Optional[torch.Tensor] = None,
    uncertainty_estimate: Optional[float] = None,
    calibration_score: Optional[float] = None,
    method_scores: Optional[Dict[str, float]] = None,
    explanation: Optional[str] = None
) -&gt; None</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="ConfidenceOutput-__replace__"><strong>__replace__</strong></a> = _replace(self, /, **changes)<span class="grey"><span class="heading-text"> from <a href="dataclasses.html">dataclasses</a></span></span></dt></dl>

<dl><dt><a name="ConfidenceOutput-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__annotations__</strong> = {'calibration_score': typing.Optional[float], 'explanation': typing.Optional[str], 'method_scores': typing.Optional[typing.Dict[str, float]], 'overall_confidence': &lt;class 'float'&gt;, 'token_confidences': typing.Optional[torch.Tensor], 'uncertainty_estimate': typing.Optional[float]}</dl>

<dl><dt><strong>__dataclass_fields__</strong> = {'calibration_score': Field(name='calibration_score',type=typing.Optio...appingproxy({}),kw_only=False,_field_type=_FIELD), 'explanation': Field(name='explanation',type=typing.Optional[st...appingproxy({}),kw_only=False,_field_type=_FIELD), 'method_scores': Field(name='method_scores',type=typing.Optional[...appingproxy({}),kw_only=False,_field_type=_FIELD), 'overall_confidence': Field(name='overall_confidence',type=&lt;class 'flo...appingproxy({}),kw_only=False,_field_type=_FIELD), 'token_confidences': Field(name='token_confidences',type=typing.Optio...appingproxy({}),kw_only=False,_field_type=_FIELD), 'uncertainty_estimate': Field(name='uncertainty_estimate',type=typing.Op...appingproxy({}),kw_only=False,_field_type=_FIELD)}</dl>

<dl><dt><strong>__dataclass_params__</strong> = _DataclassParams(init=True,repr=True,eq=True,ord...rue,kw_only=False,slots=False,weakref_slot=False)</dl>

<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__match_args__</strong> = ('overall_confidence', 'token_confidences', 'uncertainty_estimate', 'calibration_score', 'method_scores', 'explanation')</dl>

<dl><dt><strong>calibration_score</strong> = None</dl>

<dl><dt><strong>explanation</strong> = None</dl>

<dl><dt><strong>method_scores</strong> = None</dl>

<dl><dt><strong>token_confidences</strong> = None</dl>

<dl><dt><strong>uncertainty_estimate</strong> = None</dl>

</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="EntropyConfidenceEstimator">class <strong>EntropyConfidenceEstimator</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#EntropyConfidenceEstimator">EntropyConfidenceEstimator</a>(vocab_size:&nbsp;int&nbsp;=&nbsp;32000,&nbsp;temperature:&nbsp;float&nbsp;=&nbsp;1.0)<br>
&nbsp;<br>
Confidence&nbsp;estimation&nbsp;based&nbsp;on&nbsp;prediction&nbsp;entropy<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="adapters.confidence_estimator.html#EntropyConfidenceEstimator">EntropyConfidenceEstimator</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="EntropyConfidenceEstimator-__init__"><strong>__init__</strong></a>(self, vocab_size: int = 32000, temperature: float = 1.0)</dt><dd><span class="code">Initialize&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-forward"><strong>forward</strong></a>(self, logits: torch.Tensor) -&gt; torch.Tensor</dt><dd><span class="code">Compute&nbsp;confidence&nbsp;based&nbsp;on&nbsp;prediction&nbsp;entropy<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits:&nbsp;Model&nbsp;logits&nbsp;[batch_size,&nbsp;seq_len,&nbsp;vocab_size]<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Confidence&nbsp;scores&nbsp;[batch_size,&nbsp;seq_len]</span></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__annotations__</strong> = {}</dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="EntropyConfidenceEstimator-__call__"><strong>__call__</strong></a> = <a href="#EntropyConfidenceEstimator-_wrapped_call_impl">_wrapped_call_impl</a>(self, *args, **kwargs)</dt></dl>

<dl><dt><a name="EntropyConfidenceEstimator-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><span class="code">Implement&nbsp;delattr(self,&nbsp;name).</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><span class="code">Default&nbsp;dir()&nbsp;implementation.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-__getattr__"><strong>__getattr__</strong></a>(self, name: str) -&gt; Union[torch.Tensor, ForwardRef('Module')]</dt><dd><span class="code">#&nbsp;It&nbsp;is&nbsp;crucial&nbsp;that&nbsp;the&nbsp;return&nbsp;type&nbsp;is&nbsp;not&nbsp;annotated&nbsp;as&nbsp;`Any`,&nbsp;otherwise&nbsp;type&nbsp;checking<br>
#&nbsp;on&nbsp;`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;and&nbsp;all&nbsp;its&nbsp;subclasses&nbsp;is&nbsp;largely&nbsp;disabled&nbsp;as&nbsp;a&nbsp;result.&nbsp;See:<br>
#&nbsp;<a href="https://github.com/pytorch/pytorch/pull/115074">https://github.com/pytorch/pytorch/pull/115074</a></span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-__getstate__"><strong>__getstate__</strong></a>(self)</dt><dd><span class="code">Helper&nbsp;for&nbsp;pickle.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-__setattr__"><strong>__setattr__</strong></a>(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="EntropyConfidenceEstimator-add_module"><strong>add_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-apply"><strong>apply</strong></a>(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</dt><dd><span class="code">Apply&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#EntropyConfidenceEstimator-children">children</a>()``)&nbsp;as&nbsp;well&nbsp;as&nbsp;self.<br>
&nbsp;<br>
Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;@torch.no_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#EntropyConfidenceEstimator-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#EntropyConfidenceEstimator-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-bfloat16"><strong>bfloat16</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``bfloat16``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-buffers"><strong>buffers</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#EntropyConfidenceEstimator-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#EntropyConfidenceEstimator-type">type</a>(buf),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-children"><strong>children</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-compile"><strong>compile</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Compile&nbsp;this&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;using&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
This&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;`__call__`&nbsp;method&nbsp;is&nbsp;compiled&nbsp;and&nbsp;all&nbsp;arguments&nbsp;are&nbsp;passed&nbsp;as-is<br>
to&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
See&nbsp;:func:`torch.compile`&nbsp;for&nbsp;details&nbsp;on&nbsp;the&nbsp;arguments&nbsp;for&nbsp;this&nbsp;function.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-cpu"><strong>cpu</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-cuda"><strong>cuda</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-double"><strong>double</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-eval"><strong>eval</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#EntropyConfidenceEstimator-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#EntropyConfidenceEstimator-eval">eval</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-extra_repr"><strong>extra_repr</strong></a>(self) -&gt; str</dt><dd><span class="code">Return&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;re-implement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-float"><strong>float</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``float``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-get_buffer"><strong>get_buffer</strong></a>(self, target: str) -&gt; 'Tensor'</dt><dd><span class="code">Return&nbsp;the&nbsp;buffer&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;The&nbsp;buffer&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-get_extra_state"><strong>get_extra_state</strong></a>(self) -&gt; Any</dt><dd><span class="code">Return&nbsp;any&nbsp;extra&nbsp;state&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict.<br>
&nbsp;<br>
Implement&nbsp;this&nbsp;and&nbsp;a&nbsp;corresponding&nbsp;:func:`set_extra_state`&nbsp;for&nbsp;your&nbsp;module<br>
if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state.&nbsp;This&nbsp;function&nbsp;is&nbsp;called&nbsp;when&nbsp;building&nbsp;the<br>
module's&nbsp;`<a href="#EntropyConfidenceEstimator-state_dict">state_dict</a>()`.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;extra&nbsp;state&nbsp;should&nbsp;be&nbsp;picklable&nbsp;to&nbsp;ensure&nbsp;working&nbsp;serialization<br>
of&nbsp;the&nbsp;state_dict.&nbsp;We&nbsp;only&nbsp;provide&nbsp;backwards&nbsp;compatibility&nbsp;guarantees<br>
for&nbsp;serializing&nbsp;Tensors;&nbsp;other&nbsp;objects&nbsp;may&nbsp;break&nbsp;backwards&nbsp;compatibility&nbsp;if<br>
their&nbsp;serialized&nbsp;pickled&nbsp;form&nbsp;changes.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="builtins.html#object">object</a>:&nbsp;Any&nbsp;extra&nbsp;state&nbsp;to&nbsp;store&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-get_parameter"><strong>get_parameter</strong></a>(self, target: str) -&gt; 'Parameter'</dt><dd><span class="code">Return&nbsp;the&nbsp;parameter&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;Parameter<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Parameter:&nbsp;The&nbsp;Parameter&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``nn.Parameter``</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-get_submodule"><strong>get_submodule</strong></a>(self, target: str) -&gt; 'Module'</dt><dd><span class="code">Return&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(16,&nbsp;33,&nbsp;kernel_size=(3,&nbsp;3),&nbsp;stride=(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(in_features=100,&nbsp;out_features=200,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;which&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;check&nbsp;whether&nbsp;or&nbsp;not&nbsp;we&nbsp;have&nbsp;the&nbsp;``linear``&nbsp;submodule,&nbsp;we<br>
would&nbsp;call&nbsp;``<a href="#EntropyConfidenceEstimator-get_submodule">get_submodule</a>("net_b.linear")``.&nbsp;To&nbsp;check&nbsp;whether<br>
we&nbsp;have&nbsp;the&nbsp;``conv``&nbsp;submodule,&nbsp;we&nbsp;would&nbsp;call<br>
``<a href="#EntropyConfidenceEstimator-get_submodule">get_submodule</a>("net_b.net_c.conv")``.<br>
&nbsp;<br>
The&nbsp;runtime&nbsp;of&nbsp;``get_submodule``&nbsp;is&nbsp;bounded&nbsp;by&nbsp;the&nbsp;degree<br>
of&nbsp;module&nbsp;nesting&nbsp;in&nbsp;``target``.&nbsp;A&nbsp;query&nbsp;against<br>
``named_modules``&nbsp;achieves&nbsp;the&nbsp;same&nbsp;result,&nbsp;but&nbsp;it&nbsp;is&nbsp;O(N)&nbsp;in<br>
the&nbsp;number&nbsp;of&nbsp;transitive&nbsp;modules.&nbsp;So,&nbsp;for&nbsp;a&nbsp;simple&nbsp;check&nbsp;to&nbsp;see<br>
if&nbsp;some&nbsp;submodule&nbsp;exists,&nbsp;``get_submodule``&nbsp;should&nbsp;always&nbsp;be<br>
used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;The&nbsp;submodule&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-half"><strong>half</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-ipu"><strong>ipu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;IPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;IPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-load_state_dict"><strong>load_state_dict</strong></a>(
    self,
    state_dict: collections.abc.Mapping[str, typing.Any],
    strict: bool = True,
    assign: bool = False
)</dt><dd><span class="code">Copy&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into&nbsp;this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.<br>
&nbsp;<br>
If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;:attr:`assign`&nbsp;is&nbsp;``True``&nbsp;the&nbsp;optimizer&nbsp;must&nbsp;be&nbsp;created&nbsp;after<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;call&nbsp;to&nbsp;:attr:`load_state_dict`&nbsp;unless<br>
&nbsp;&nbsp;&nbsp;&nbsp;:func:`~torch.__future__.get_swap_module_params_on_conversion`&nbsp;is&nbsp;``True``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;&nbsp;&nbsp;&nbsp;assign&nbsp;(bool,&nbsp;optional):&nbsp;When&nbsp;set&nbsp;to&nbsp;``False``,&nbsp;the&nbsp;properties&nbsp;of&nbsp;the&nbsp;tensors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;current&nbsp;module&nbsp;are&nbsp;preserved&nbsp;whereas&nbsp;setting&nbsp;it&nbsp;to&nbsp;``True``&nbsp;preserves<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;properties&nbsp;of&nbsp;the&nbsp;Tensors&nbsp;in&nbsp;the&nbsp;state&nbsp;dict.&nbsp;The&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exception&nbsp;is&nbsp;the&nbsp;``requires_grad``&nbsp;field&nbsp;of&nbsp;:class:`~torch.nn.Parameter`s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;which&nbsp;the&nbsp;value&nbsp;from&nbsp;the&nbsp;module&nbsp;is&nbsp;preserved.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;any&nbsp;keys&nbsp;that&nbsp;are&nbsp;expected<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;missing&nbsp;from&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;keys&nbsp;that&nbsp;are&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expected&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;present&nbsp;in&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;a&nbsp;parameter&nbsp;or&nbsp;buffer&nbsp;is&nbsp;registered&nbsp;as&nbsp;``None``&nbsp;and&nbsp;its&nbsp;corresponding&nbsp;key<br>
&nbsp;&nbsp;&nbsp;&nbsp;exists&nbsp;in&nbsp;:attr:`state_dict`,&nbsp;:meth:`load_state_dict`&nbsp;will&nbsp;raise&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;``RuntimeError``.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-modules"><strong>modules</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#EntropyConfidenceEstimator-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-mtia"><strong>mtia</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;MTIA.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;MTIA&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-named_buffers"><strong>named_buffers</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool,&nbsp;optional):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;buffers&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#EntropyConfidenceEstimator-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-named_children"><strong>named_children</strong></a>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#EntropyConfidenceEstimator-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-named_modules"><strong>named_modules</strong></a>(
    self,
    memo: Optional[set['Module']] = None,
    prefix: str = '',
    remove_duplicate: bool = True
)</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;memo:&nbsp;a&nbsp;memo&nbsp;to&nbsp;store&nbsp;the&nbsp;set&nbsp;of&nbsp;modules&nbsp;already&nbsp;added&nbsp;to&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix:&nbsp;a&nbsp;prefix&nbsp;that&nbsp;will&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate:&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;module&nbsp;instances&nbsp;in&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;not<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#EntropyConfidenceEstimator-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-named_parameters"><strong>named_parameters</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#EntropyConfidenceEstimator-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-parameters"><strong>parameters</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#EntropyConfidenceEstimator-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#EntropyConfidenceEstimator-type">type</a>(param),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_backward_hook"><strong>register_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;deprecated&nbsp;in&nbsp;favor&nbsp;of&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.register_full_backward_hook`&nbsp;and<br>
the&nbsp;behavior&nbsp;of&nbsp;this&nbsp;function&nbsp;will&nbsp;change&nbsp;in&nbsp;future&nbsp;versions.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_buffer"><strong>register_buffer</strong></a>(
    self,
    name: str,
    tensor: Optional[torch.Tensor],
    persistent: bool = True
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;module's&nbsp;state.&nbsp;Buffers,&nbsp;by<br>
default,&nbsp;are&nbsp;persistent&nbsp;and&nbsp;will&nbsp;be&nbsp;saved&nbsp;alongside&nbsp;parameters.&nbsp;This<br>
behavior&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;setting&nbsp;:attr:`persistent`&nbsp;to&nbsp;``False``.&nbsp;The<br>
only&nbsp;difference&nbsp;between&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;and&nbsp;a&nbsp;non-persistent&nbsp;buffer<br>
is&nbsp;that&nbsp;the&nbsp;latter&nbsp;will&nbsp;not&nbsp;be&nbsp;a&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
:attr:`state_dict`.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor&nbsp;or&nbsp;None):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.&nbsp;If&nbsp;``None``,&nbsp;then&nbsp;operations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;run&nbsp;on&nbsp;buffers,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;buffer&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the&nbsp;module's&nbsp;:attr:`state_dict`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;(bool):&nbsp;whether&nbsp;the&nbsp;buffer&nbsp;is&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:attr:`state_dict`.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#EntropyConfidenceEstimator-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_forward_hook"><strong>register_forward_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False,
    always_call: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``False``&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but&nbsp;it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on<br>
forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after&nbsp;:func:`forward`&nbsp;is&nbsp;called.&nbsp;The&nbsp;hook<br>
should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``True``,&nbsp;the&nbsp;forward&nbsp;hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
``kwargs``&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function&nbsp;and&nbsp;be&nbsp;expected&nbsp;to&nbsp;return&nbsp;the<br>
output&nbsp;possibly&nbsp;modified.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;always_call&nbsp;(bool):&nbsp;If&nbsp;``True``&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;run&nbsp;regardless&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;whether&nbsp;an&nbsp;exception&nbsp;is&nbsp;raised&nbsp;while&nbsp;calling&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
&nbsp;<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;false&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a&nbsp;single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the<br>
hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple&nbsp;if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned<br>
(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the<br>
following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;true,&nbsp;the&nbsp;forward&nbsp;pre-hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.&nbsp;And&nbsp;if&nbsp;the&nbsp;hook&nbsp;modifies&nbsp;the<br>
input,&nbsp;both&nbsp;the&nbsp;args&nbsp;and&nbsp;kwargs&nbsp;should&nbsp;be&nbsp;returned.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have<br>
the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;a&nbsp;tuple&nbsp;of&nbsp;modified&nbsp;input&nbsp;and&nbsp;kwargs<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the&nbsp;kwargs<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_full_backward_hook"><strong>register_full_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;a&nbsp;module<br>
are&nbsp;computed,&nbsp;i.e.&nbsp;the&nbsp;hook&nbsp;will&nbsp;execute&nbsp;if&nbsp;and&nbsp;only&nbsp;if&nbsp;the&nbsp;gradients&nbsp;with<br>
respect&nbsp;to&nbsp;module&nbsp;outputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following<br>
signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple(Tensor)&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;are&nbsp;tuples&nbsp;that&nbsp;contain&nbsp;the&nbsp;gradients<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs&nbsp;respectively.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;:attr:`grad_input`&nbsp;will&nbsp;only&nbsp;correspond&nbsp;to&nbsp;the&nbsp;inputs&nbsp;given<br>
as&nbsp;positional&nbsp;arguments&nbsp;and&nbsp;all&nbsp;kwarg&nbsp;arguments&nbsp;are&nbsp;ignored.&nbsp;Entries<br>
in&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for&nbsp;all&nbsp;non-Tensor<br>
arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;or&nbsp;outputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_full_backward_pre_hook"><strong>register_full_backward_pre_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;for&nbsp;the&nbsp;module&nbsp;are&nbsp;computed.<br>
The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple[Tensor]&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_output`&nbsp;is&nbsp;a&nbsp;tuple.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;output&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_output`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;Entries&nbsp;in&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for<br>
all&nbsp;non-Tensor&nbsp;arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_load_state_dict_post_hook"><strong>register_load_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;after&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;incompatible_keys)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;``module``&nbsp;argument&nbsp;is&nbsp;the&nbsp;current&nbsp;module&nbsp;that&nbsp;this&nbsp;hook&nbsp;is&nbsp;registered<br>
on,&nbsp;and&nbsp;the&nbsp;``incompatible_keys``&nbsp;argument&nbsp;is&nbsp;a&nbsp;``NamedTuple``&nbsp;consisting<br>
of&nbsp;attributes&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``.&nbsp;``missing_keys``<br>
is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys&nbsp;and<br>
``unexpected_keys``&nbsp;is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys.<br>
&nbsp;<br>
The&nbsp;given&nbsp;incompatible_keys&nbsp;can&nbsp;be&nbsp;modified&nbsp;inplace&nbsp;if&nbsp;needed.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;checks&nbsp;performed&nbsp;when&nbsp;calling&nbsp;:func:`load_state_dict`&nbsp;with<br>
``strict=True``&nbsp;are&nbsp;affected&nbsp;by&nbsp;modifications&nbsp;the&nbsp;hook&nbsp;makes&nbsp;to<br>
``missing_keys``&nbsp;or&nbsp;``unexpected_keys``,&nbsp;as&nbsp;expected.&nbsp;Additions&nbsp;to&nbsp;either<br>
set&nbsp;of&nbsp;keys&nbsp;will&nbsp;result&nbsp;in&nbsp;an&nbsp;error&nbsp;being&nbsp;thrown&nbsp;when&nbsp;``strict=True``,&nbsp;and<br>
clearing&nbsp;out&nbsp;both&nbsp;missing&nbsp;and&nbsp;unexpected&nbsp;keys&nbsp;will&nbsp;avoid&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_load_state_dict_pre_hook"><strong>register_load_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;before&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata,&nbsp;strict,&nbsp;missing_keys,&nbsp;unexpected_keys,&nbsp;error_msgs)&nbsp;-&gt;&nbsp;None&nbsp;&nbsp;#&nbsp;noqa:&nbsp;B950<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;Callable&nbsp;hook&nbsp;that&nbsp;will&nbsp;be&nbsp;invoked&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loading&nbsp;the&nbsp;state&nbsp;dict.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_module"><strong>register_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Alias&nbsp;for&nbsp;:func:`add_module`.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_parameter"><strong>register_parameter</strong></a>(
    self,
    name: str,
    param: Optional[torch.nn.parameter.Parameter]
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter&nbsp;or&nbsp;None):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``None``,&nbsp;then&nbsp;operations&nbsp;that&nbsp;run&nbsp;on&nbsp;parameters,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,&nbsp;the&nbsp;parameter&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;module's&nbsp;:attr:`state_dict`.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_state_dict_post_hook"><strong>register_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;modify&nbsp;the&nbsp;``state_dict``&nbsp;inplace.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-register_state_dict_pre_hook"><strong>register_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;prefix,&nbsp;keep_vars)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;perform&nbsp;pre-processing&nbsp;before&nbsp;the&nbsp;``state_dict``<br>
call&nbsp;is&nbsp;made.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-requires_grad_"><strong>requires_grad_</strong></a>(self: ~T, requires_grad: bool = True) -&gt; ~T</dt><dd><span class="code">Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#EntropyConfidenceEstimator-requires_grad_">requires_grad_</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-set_extra_state"><strong>set_extra_state</strong></a>(self, state: Any) -&gt; None</dt><dd><span class="code">Set&nbsp;extra&nbsp;state&nbsp;contained&nbsp;in&nbsp;the&nbsp;loaded&nbsp;`state_dict`.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;called&nbsp;from&nbsp;:func:`load_state_dict`&nbsp;to&nbsp;handle&nbsp;any&nbsp;extra&nbsp;state<br>
found&nbsp;within&nbsp;the&nbsp;`state_dict`.&nbsp;Implement&nbsp;this&nbsp;function&nbsp;and&nbsp;a&nbsp;corresponding<br>
:func:`get_extra_state`&nbsp;for&nbsp;your&nbsp;module&nbsp;if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state&nbsp;within&nbsp;its<br>
`state_dict`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;(dict):&nbsp;Extra&nbsp;state&nbsp;from&nbsp;the&nbsp;`state_dict`</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-set_submodule"><strong>set_submodule</strong></a>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</dt><dd><span class="code">Set&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``False``&nbsp;(default),&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error&nbsp;if<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;submodule&nbsp;does&nbsp;not&nbsp;exist.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(3,&nbsp;3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;override&nbsp;the&nbsp;``Conv2d``&nbsp;with&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Linear``,&nbsp;you<br>
could&nbsp;call&nbsp;``<a href="#EntropyConfidenceEstimator-set_submodule">set_submodule</a>("net_b.net_c.conv",&nbsp;nn.Linear(1,&nbsp;1))``<br>
where&nbsp;``strict``&nbsp;could&nbsp;be&nbsp;``True``&nbsp;or&nbsp;``False``<br>
&nbsp;<br>
To&nbsp;add&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Conv2d``&nbsp;to&nbsp;the&nbsp;existing&nbsp;``net_b``&nbsp;module,<br>
you&nbsp;would&nbsp;call&nbsp;``<a href="#EntropyConfidenceEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1))``.<br>
&nbsp;<br>
In&nbsp;the&nbsp;above&nbsp;if&nbsp;you&nbsp;set&nbsp;``strict=True``&nbsp;and&nbsp;call<br>
``<a href="#EntropyConfidenceEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1),&nbsp;strict=True)``,&nbsp;an&nbsp;AttributeError<br>
will&nbsp;be&nbsp;raised&nbsp;because&nbsp;``net_b``&nbsp;does&nbsp;not&nbsp;have&nbsp;a&nbsp;submodule&nbsp;named&nbsp;``conv``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;module:&nbsp;The&nbsp;module&nbsp;to&nbsp;set&nbsp;the&nbsp;submodule&nbsp;to.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;If&nbsp;``False``,&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;the&nbsp;submodule&nbsp;doesn't&nbsp;already&nbsp;exist.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;``target``&nbsp;string&nbsp;is&nbsp;empty&nbsp;or&nbsp;if&nbsp;``module``&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;``target``&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-share_memory"><strong>share_memory</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">See&nbsp;:meth:`torch.Tensor.share_memory_`.</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-state_dict"><strong>state_dict</strong></a>(self, *args, destination=None, prefix='', keep_vars=False)</dt><dd><span class="code">Return&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;references&nbsp;to&nbsp;the&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
Parameters&nbsp;and&nbsp;buffers&nbsp;set&nbsp;to&nbsp;``None``&nbsp;are&nbsp;not&nbsp;included.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;returned&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;a&nbsp;shallow&nbsp;copy.&nbsp;It&nbsp;contains&nbsp;references<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;module's&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;``<a href="#EntropyConfidenceEstimator-state_dict">state_dict</a>()``&nbsp;also&nbsp;accepts&nbsp;positional&nbsp;arguments&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;``destination``,&nbsp;``prefix``&nbsp;and&nbsp;``keep_vars``&nbsp;in&nbsp;order.&nbsp;However,<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;being&nbsp;deprecated&nbsp;and&nbsp;keyword&nbsp;arguments&nbsp;will&nbsp;be&nbsp;enforced&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;future&nbsp;releases.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;avoid&nbsp;the&nbsp;use&nbsp;of&nbsp;argument&nbsp;``destination``&nbsp;as&nbsp;it&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;designed&nbsp;for&nbsp;end-users.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;destination&nbsp;(dict,&nbsp;optional):&nbsp;If&nbsp;provided,&nbsp;the&nbsp;state&nbsp;of&nbsp;module&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;updated&nbsp;into&nbsp;the&nbsp;dict&nbsp;and&nbsp;the&nbsp;same&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;an&nbsp;``OrderedDict``&nbsp;will&nbsp;be&nbsp;created&nbsp;and&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``None``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str,&nbsp;optional):&nbsp;a&nbsp;prefix&nbsp;added&nbsp;to&nbsp;parameter&nbsp;and&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;names&nbsp;to&nbsp;compose&nbsp;the&nbsp;keys&nbsp;in&nbsp;state_dict.&nbsp;Default:&nbsp;``''``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;keep_vars&nbsp;(bool,&nbsp;optional):&nbsp;by&nbsp;default&nbsp;the&nbsp;:class:`~torch.Tensor`&nbsp;s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returned&nbsp;in&nbsp;the&nbsp;state&nbsp;dict&nbsp;are&nbsp;detached&nbsp;from&nbsp;autograd.&nbsp;If&nbsp;it's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;to&nbsp;``True``,&nbsp;detaching&nbsp;will&nbsp;not&nbsp;be&nbsp;performed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#EntropyConfidenceEstimator-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Move&nbsp;and/or&nbsp;cast&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#EntropyConfidenceEstimator-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#EntropyConfidenceEstimator-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#EntropyConfidenceEstimator-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#EntropyConfidenceEstimator-to">to</a>(memory_format=torch.channels_last)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;:attr:`dtype`\&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;dtype&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_format&nbsp;(:class:`torch.memory_format`):&nbsp;the&nbsp;desired&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format&nbsp;for&nbsp;4D&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module&nbsp;(keyword<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;argument)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+IGNORE_WANT("non-deterministic")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#EntropyConfidenceEstimator-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+REQUIRES(env:TORCH_DOCTEST_CUDA1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#EntropyConfidenceEstimator-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#EntropyConfidenceEstimator-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2,&nbsp;bias=None).<a href="#EntropyConfidenceEstimator-to">to</a>(torch.cdouble)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.3741+0.j,&nbsp;&nbsp;0.2382+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;0.5593+0.j,&nbsp;-0.4443+0.j]],&nbsp;dtype=torch.complex128)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear(torch.ones(3,&nbsp;2,&nbsp;dtype=torch.cdouble))<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j]],&nbsp;dtype=torch.complex128)</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-to_empty"><strong>to_empty</strong></a>(
    self: ~T,
    *,
    device: Union[int, str, torch.device, NoneType],
    recurse: bool = True
) -&gt; ~T</dt><dd><span class="code">Move&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;specified&nbsp;device&nbsp;without&nbsp;copying&nbsp;storage.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;The&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;Whether&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;of&nbsp;submodules&nbsp;should<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;recursively&nbsp;moved&nbsp;to&nbsp;the&nbsp;specified&nbsp;device.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-train"><strong>train</strong></a>(self: ~T, mode: bool = True) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.,&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-type"><strong>type</strong></a>(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-xpu"><strong>xpu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;XPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;XPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="EntropyConfidenceEstimator-zero_grad"><strong>zero_grad</strong></a>(self, set_to_none: bool = True) -&gt; None</dt><dd><span class="code">Reset&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters.<br>
&nbsp;<br>
See&nbsp;similar&nbsp;function&nbsp;under&nbsp;:class:`torch.optim.Optimizer`&nbsp;for&nbsp;more&nbsp;context.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;set_to_none&nbsp;(bool):&nbsp;instead&nbsp;of&nbsp;setting&nbsp;to&nbsp;zero,&nbsp;set&nbsp;the&nbsp;grads&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;:meth:`torch.optim.Optimizer.zero_grad`&nbsp;for&nbsp;details.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>T_destination</strong> = ~T_destination</dl>

<dl><dt><strong>call_super_init</strong> = False</dl>

<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="MonteCarloDropoutEstimator">class <strong>MonteCarloDropoutEstimator</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#MonteCarloDropoutEstimator">MonteCarloDropoutEstimator</a>(dropout_rate:&nbsp;float&nbsp;=&nbsp;0.1,&nbsp;num_samples:&nbsp;int&nbsp;=&nbsp;10)<br>
&nbsp;<br>
Monte&nbsp;Carlo&nbsp;Dropout&nbsp;for&nbsp;uncertainty&nbsp;estimation<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="adapters.confidence_estimator.html#MonteCarloDropoutEstimator">MonteCarloDropoutEstimator</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="MonteCarloDropoutEstimator-__init__"><strong>__init__</strong></a>(self, dropout_rate: float = 0.1, num_samples: int = 10)</dt><dd><span class="code">Initialize&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-forward"><strong>forward</strong></a>(
    self,
    hidden_states: torch.Tensor,
    prediction_head: torch.nn.modules.module.Module
) -&gt; Tuple[torch.Tensor, torch.Tensor]</dt><dd><span class="code">Estimate&nbsp;uncertainty&nbsp;using&nbsp;Monte&nbsp;Carlo&nbsp;Dropout<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hidden_states:&nbsp;Hidden&nbsp;representations&nbsp;[batch_size,&nbsp;seq_len,&nbsp;hidden_dim]<br>
&nbsp;&nbsp;&nbsp;&nbsp;prediction_head:&nbsp;Model&nbsp;prediction&nbsp;head<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Mean&nbsp;predictions&nbsp;and&nbsp;uncertainty&nbsp;estimates</span></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__annotations__</strong> = {}</dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="MonteCarloDropoutEstimator-__call__"><strong>__call__</strong></a> = <a href="#MonteCarloDropoutEstimator-_wrapped_call_impl">_wrapped_call_impl</a>(self, *args, **kwargs)</dt></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><span class="code">Implement&nbsp;delattr(self,&nbsp;name).</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><span class="code">Default&nbsp;dir()&nbsp;implementation.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-__getattr__"><strong>__getattr__</strong></a>(self, name: str) -&gt; Union[torch.Tensor, ForwardRef('Module')]</dt><dd><span class="code">#&nbsp;It&nbsp;is&nbsp;crucial&nbsp;that&nbsp;the&nbsp;return&nbsp;type&nbsp;is&nbsp;not&nbsp;annotated&nbsp;as&nbsp;`Any`,&nbsp;otherwise&nbsp;type&nbsp;checking<br>
#&nbsp;on&nbsp;`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;and&nbsp;all&nbsp;its&nbsp;subclasses&nbsp;is&nbsp;largely&nbsp;disabled&nbsp;as&nbsp;a&nbsp;result.&nbsp;See:<br>
#&nbsp;<a href="https://github.com/pytorch/pytorch/pull/115074">https://github.com/pytorch/pytorch/pull/115074</a></span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-__getstate__"><strong>__getstate__</strong></a>(self)</dt><dd><span class="code">Helper&nbsp;for&nbsp;pickle.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-__setattr__"><strong>__setattr__</strong></a>(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-add_module"><strong>add_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-apply"><strong>apply</strong></a>(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</dt><dd><span class="code">Apply&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#MonteCarloDropoutEstimator-children">children</a>()``)&nbsp;as&nbsp;well&nbsp;as&nbsp;self.<br>
&nbsp;<br>
Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;@torch.no_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#MonteCarloDropoutEstimator-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#MonteCarloDropoutEstimator-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-bfloat16"><strong>bfloat16</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``bfloat16``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-buffers"><strong>buffers</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#MonteCarloDropoutEstimator-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#MonteCarloDropoutEstimator-type">type</a>(buf),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-children"><strong>children</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-compile"><strong>compile</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Compile&nbsp;this&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;using&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
This&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;`__call__`&nbsp;method&nbsp;is&nbsp;compiled&nbsp;and&nbsp;all&nbsp;arguments&nbsp;are&nbsp;passed&nbsp;as-is<br>
to&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
See&nbsp;:func:`torch.compile`&nbsp;for&nbsp;details&nbsp;on&nbsp;the&nbsp;arguments&nbsp;for&nbsp;this&nbsp;function.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-cpu"><strong>cpu</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-cuda"><strong>cuda</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-double"><strong>double</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-eval"><strong>eval</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#MonteCarloDropoutEstimator-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#MonteCarloDropoutEstimator-eval">eval</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-extra_repr"><strong>extra_repr</strong></a>(self) -&gt; str</dt><dd><span class="code">Return&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;re-implement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-float"><strong>float</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``float``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-get_buffer"><strong>get_buffer</strong></a>(self, target: str) -&gt; 'Tensor'</dt><dd><span class="code">Return&nbsp;the&nbsp;buffer&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;The&nbsp;buffer&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-get_extra_state"><strong>get_extra_state</strong></a>(self) -&gt; Any</dt><dd><span class="code">Return&nbsp;any&nbsp;extra&nbsp;state&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict.<br>
&nbsp;<br>
Implement&nbsp;this&nbsp;and&nbsp;a&nbsp;corresponding&nbsp;:func:`set_extra_state`&nbsp;for&nbsp;your&nbsp;module<br>
if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state.&nbsp;This&nbsp;function&nbsp;is&nbsp;called&nbsp;when&nbsp;building&nbsp;the<br>
module's&nbsp;`<a href="#MonteCarloDropoutEstimator-state_dict">state_dict</a>()`.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;extra&nbsp;state&nbsp;should&nbsp;be&nbsp;picklable&nbsp;to&nbsp;ensure&nbsp;working&nbsp;serialization<br>
of&nbsp;the&nbsp;state_dict.&nbsp;We&nbsp;only&nbsp;provide&nbsp;backwards&nbsp;compatibility&nbsp;guarantees<br>
for&nbsp;serializing&nbsp;Tensors;&nbsp;other&nbsp;objects&nbsp;may&nbsp;break&nbsp;backwards&nbsp;compatibility&nbsp;if<br>
their&nbsp;serialized&nbsp;pickled&nbsp;form&nbsp;changes.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="builtins.html#object">object</a>:&nbsp;Any&nbsp;extra&nbsp;state&nbsp;to&nbsp;store&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-get_parameter"><strong>get_parameter</strong></a>(self, target: str) -&gt; 'Parameter'</dt><dd><span class="code">Return&nbsp;the&nbsp;parameter&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;Parameter<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Parameter:&nbsp;The&nbsp;Parameter&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``nn.Parameter``</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-get_submodule"><strong>get_submodule</strong></a>(self, target: str) -&gt; 'Module'</dt><dd><span class="code">Return&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(16,&nbsp;33,&nbsp;kernel_size=(3,&nbsp;3),&nbsp;stride=(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(in_features=100,&nbsp;out_features=200,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;which&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;check&nbsp;whether&nbsp;or&nbsp;not&nbsp;we&nbsp;have&nbsp;the&nbsp;``linear``&nbsp;submodule,&nbsp;we<br>
would&nbsp;call&nbsp;``<a href="#MonteCarloDropoutEstimator-get_submodule">get_submodule</a>("net_b.linear")``.&nbsp;To&nbsp;check&nbsp;whether<br>
we&nbsp;have&nbsp;the&nbsp;``conv``&nbsp;submodule,&nbsp;we&nbsp;would&nbsp;call<br>
``<a href="#MonteCarloDropoutEstimator-get_submodule">get_submodule</a>("net_b.net_c.conv")``.<br>
&nbsp;<br>
The&nbsp;runtime&nbsp;of&nbsp;``get_submodule``&nbsp;is&nbsp;bounded&nbsp;by&nbsp;the&nbsp;degree<br>
of&nbsp;module&nbsp;nesting&nbsp;in&nbsp;``target``.&nbsp;A&nbsp;query&nbsp;against<br>
``named_modules``&nbsp;achieves&nbsp;the&nbsp;same&nbsp;result,&nbsp;but&nbsp;it&nbsp;is&nbsp;O(N)&nbsp;in<br>
the&nbsp;number&nbsp;of&nbsp;transitive&nbsp;modules.&nbsp;So,&nbsp;for&nbsp;a&nbsp;simple&nbsp;check&nbsp;to&nbsp;see<br>
if&nbsp;some&nbsp;submodule&nbsp;exists,&nbsp;``get_submodule``&nbsp;should&nbsp;always&nbsp;be<br>
used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;The&nbsp;submodule&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-half"><strong>half</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-ipu"><strong>ipu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;IPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;IPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-load_state_dict"><strong>load_state_dict</strong></a>(
    self,
    state_dict: collections.abc.Mapping[str, typing.Any],
    strict: bool = True,
    assign: bool = False
)</dt><dd><span class="code">Copy&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into&nbsp;this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.<br>
&nbsp;<br>
If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;:attr:`assign`&nbsp;is&nbsp;``True``&nbsp;the&nbsp;optimizer&nbsp;must&nbsp;be&nbsp;created&nbsp;after<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;call&nbsp;to&nbsp;:attr:`load_state_dict`&nbsp;unless<br>
&nbsp;&nbsp;&nbsp;&nbsp;:func:`~torch.__future__.get_swap_module_params_on_conversion`&nbsp;is&nbsp;``True``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;&nbsp;&nbsp;&nbsp;assign&nbsp;(bool,&nbsp;optional):&nbsp;When&nbsp;set&nbsp;to&nbsp;``False``,&nbsp;the&nbsp;properties&nbsp;of&nbsp;the&nbsp;tensors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;current&nbsp;module&nbsp;are&nbsp;preserved&nbsp;whereas&nbsp;setting&nbsp;it&nbsp;to&nbsp;``True``&nbsp;preserves<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;properties&nbsp;of&nbsp;the&nbsp;Tensors&nbsp;in&nbsp;the&nbsp;state&nbsp;dict.&nbsp;The&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exception&nbsp;is&nbsp;the&nbsp;``requires_grad``&nbsp;field&nbsp;of&nbsp;:class:`~torch.nn.Parameter`s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;which&nbsp;the&nbsp;value&nbsp;from&nbsp;the&nbsp;module&nbsp;is&nbsp;preserved.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;any&nbsp;keys&nbsp;that&nbsp;are&nbsp;expected<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;missing&nbsp;from&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;keys&nbsp;that&nbsp;are&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expected&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;present&nbsp;in&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;a&nbsp;parameter&nbsp;or&nbsp;buffer&nbsp;is&nbsp;registered&nbsp;as&nbsp;``None``&nbsp;and&nbsp;its&nbsp;corresponding&nbsp;key<br>
&nbsp;&nbsp;&nbsp;&nbsp;exists&nbsp;in&nbsp;:attr:`state_dict`,&nbsp;:meth:`load_state_dict`&nbsp;will&nbsp;raise&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;``RuntimeError``.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-modules"><strong>modules</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#MonteCarloDropoutEstimator-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-mtia"><strong>mtia</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;MTIA.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;MTIA&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-named_buffers"><strong>named_buffers</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool,&nbsp;optional):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;buffers&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#MonteCarloDropoutEstimator-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-named_children"><strong>named_children</strong></a>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#MonteCarloDropoutEstimator-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-named_modules"><strong>named_modules</strong></a>(
    self,
    memo: Optional[set['Module']] = None,
    prefix: str = '',
    remove_duplicate: bool = True
)</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;memo:&nbsp;a&nbsp;memo&nbsp;to&nbsp;store&nbsp;the&nbsp;set&nbsp;of&nbsp;modules&nbsp;already&nbsp;added&nbsp;to&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix:&nbsp;a&nbsp;prefix&nbsp;that&nbsp;will&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate:&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;module&nbsp;instances&nbsp;in&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;not<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#MonteCarloDropoutEstimator-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-named_parameters"><strong>named_parameters</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#MonteCarloDropoutEstimator-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-parameters"><strong>parameters</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#MonteCarloDropoutEstimator-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#MonteCarloDropoutEstimator-type">type</a>(param),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_backward_hook"><strong>register_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;deprecated&nbsp;in&nbsp;favor&nbsp;of&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.register_full_backward_hook`&nbsp;and<br>
the&nbsp;behavior&nbsp;of&nbsp;this&nbsp;function&nbsp;will&nbsp;change&nbsp;in&nbsp;future&nbsp;versions.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_buffer"><strong>register_buffer</strong></a>(
    self,
    name: str,
    tensor: Optional[torch.Tensor],
    persistent: bool = True
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;module's&nbsp;state.&nbsp;Buffers,&nbsp;by<br>
default,&nbsp;are&nbsp;persistent&nbsp;and&nbsp;will&nbsp;be&nbsp;saved&nbsp;alongside&nbsp;parameters.&nbsp;This<br>
behavior&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;setting&nbsp;:attr:`persistent`&nbsp;to&nbsp;``False``.&nbsp;The<br>
only&nbsp;difference&nbsp;between&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;and&nbsp;a&nbsp;non-persistent&nbsp;buffer<br>
is&nbsp;that&nbsp;the&nbsp;latter&nbsp;will&nbsp;not&nbsp;be&nbsp;a&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
:attr:`state_dict`.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor&nbsp;or&nbsp;None):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.&nbsp;If&nbsp;``None``,&nbsp;then&nbsp;operations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;run&nbsp;on&nbsp;buffers,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;buffer&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the&nbsp;module's&nbsp;:attr:`state_dict`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;(bool):&nbsp;whether&nbsp;the&nbsp;buffer&nbsp;is&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:attr:`state_dict`.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#MonteCarloDropoutEstimator-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_forward_hook"><strong>register_forward_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False,
    always_call: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``False``&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but&nbsp;it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on<br>
forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after&nbsp;:func:`forward`&nbsp;is&nbsp;called.&nbsp;The&nbsp;hook<br>
should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``True``,&nbsp;the&nbsp;forward&nbsp;hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
``kwargs``&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function&nbsp;and&nbsp;be&nbsp;expected&nbsp;to&nbsp;return&nbsp;the<br>
output&nbsp;possibly&nbsp;modified.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;always_call&nbsp;(bool):&nbsp;If&nbsp;``True``&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;run&nbsp;regardless&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;whether&nbsp;an&nbsp;exception&nbsp;is&nbsp;raised&nbsp;while&nbsp;calling&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
&nbsp;<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;false&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a&nbsp;single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the<br>
hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple&nbsp;if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned<br>
(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the<br>
following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;true,&nbsp;the&nbsp;forward&nbsp;pre-hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.&nbsp;And&nbsp;if&nbsp;the&nbsp;hook&nbsp;modifies&nbsp;the<br>
input,&nbsp;both&nbsp;the&nbsp;args&nbsp;and&nbsp;kwargs&nbsp;should&nbsp;be&nbsp;returned.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have<br>
the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;a&nbsp;tuple&nbsp;of&nbsp;modified&nbsp;input&nbsp;and&nbsp;kwargs<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the&nbsp;kwargs<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_full_backward_hook"><strong>register_full_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;a&nbsp;module<br>
are&nbsp;computed,&nbsp;i.e.&nbsp;the&nbsp;hook&nbsp;will&nbsp;execute&nbsp;if&nbsp;and&nbsp;only&nbsp;if&nbsp;the&nbsp;gradients&nbsp;with<br>
respect&nbsp;to&nbsp;module&nbsp;outputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following<br>
signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple(Tensor)&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;are&nbsp;tuples&nbsp;that&nbsp;contain&nbsp;the&nbsp;gradients<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs&nbsp;respectively.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;:attr:`grad_input`&nbsp;will&nbsp;only&nbsp;correspond&nbsp;to&nbsp;the&nbsp;inputs&nbsp;given<br>
as&nbsp;positional&nbsp;arguments&nbsp;and&nbsp;all&nbsp;kwarg&nbsp;arguments&nbsp;are&nbsp;ignored.&nbsp;Entries<br>
in&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for&nbsp;all&nbsp;non-Tensor<br>
arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;or&nbsp;outputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_full_backward_pre_hook"><strong>register_full_backward_pre_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;for&nbsp;the&nbsp;module&nbsp;are&nbsp;computed.<br>
The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple[Tensor]&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_output`&nbsp;is&nbsp;a&nbsp;tuple.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;output&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_output`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;Entries&nbsp;in&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for<br>
all&nbsp;non-Tensor&nbsp;arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_load_state_dict_post_hook"><strong>register_load_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;after&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;incompatible_keys)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;``module``&nbsp;argument&nbsp;is&nbsp;the&nbsp;current&nbsp;module&nbsp;that&nbsp;this&nbsp;hook&nbsp;is&nbsp;registered<br>
on,&nbsp;and&nbsp;the&nbsp;``incompatible_keys``&nbsp;argument&nbsp;is&nbsp;a&nbsp;``NamedTuple``&nbsp;consisting<br>
of&nbsp;attributes&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``.&nbsp;``missing_keys``<br>
is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys&nbsp;and<br>
``unexpected_keys``&nbsp;is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys.<br>
&nbsp;<br>
The&nbsp;given&nbsp;incompatible_keys&nbsp;can&nbsp;be&nbsp;modified&nbsp;inplace&nbsp;if&nbsp;needed.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;checks&nbsp;performed&nbsp;when&nbsp;calling&nbsp;:func:`load_state_dict`&nbsp;with<br>
``strict=True``&nbsp;are&nbsp;affected&nbsp;by&nbsp;modifications&nbsp;the&nbsp;hook&nbsp;makes&nbsp;to<br>
``missing_keys``&nbsp;or&nbsp;``unexpected_keys``,&nbsp;as&nbsp;expected.&nbsp;Additions&nbsp;to&nbsp;either<br>
set&nbsp;of&nbsp;keys&nbsp;will&nbsp;result&nbsp;in&nbsp;an&nbsp;error&nbsp;being&nbsp;thrown&nbsp;when&nbsp;``strict=True``,&nbsp;and<br>
clearing&nbsp;out&nbsp;both&nbsp;missing&nbsp;and&nbsp;unexpected&nbsp;keys&nbsp;will&nbsp;avoid&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_load_state_dict_pre_hook"><strong>register_load_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;before&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata,&nbsp;strict,&nbsp;missing_keys,&nbsp;unexpected_keys,&nbsp;error_msgs)&nbsp;-&gt;&nbsp;None&nbsp;&nbsp;#&nbsp;noqa:&nbsp;B950<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;Callable&nbsp;hook&nbsp;that&nbsp;will&nbsp;be&nbsp;invoked&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loading&nbsp;the&nbsp;state&nbsp;dict.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_module"><strong>register_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Alias&nbsp;for&nbsp;:func:`add_module`.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_parameter"><strong>register_parameter</strong></a>(
    self,
    name: str,
    param: Optional[torch.nn.parameter.Parameter]
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter&nbsp;or&nbsp;None):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``None``,&nbsp;then&nbsp;operations&nbsp;that&nbsp;run&nbsp;on&nbsp;parameters,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,&nbsp;the&nbsp;parameter&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;module's&nbsp;:attr:`state_dict`.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_state_dict_post_hook"><strong>register_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;modify&nbsp;the&nbsp;``state_dict``&nbsp;inplace.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-register_state_dict_pre_hook"><strong>register_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;prefix,&nbsp;keep_vars)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;perform&nbsp;pre-processing&nbsp;before&nbsp;the&nbsp;``state_dict``<br>
call&nbsp;is&nbsp;made.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-requires_grad_"><strong>requires_grad_</strong></a>(self: ~T, requires_grad: bool = True) -&gt; ~T</dt><dd><span class="code">Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#MonteCarloDropoutEstimator-requires_grad_">requires_grad_</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-set_extra_state"><strong>set_extra_state</strong></a>(self, state: Any) -&gt; None</dt><dd><span class="code">Set&nbsp;extra&nbsp;state&nbsp;contained&nbsp;in&nbsp;the&nbsp;loaded&nbsp;`state_dict`.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;called&nbsp;from&nbsp;:func:`load_state_dict`&nbsp;to&nbsp;handle&nbsp;any&nbsp;extra&nbsp;state<br>
found&nbsp;within&nbsp;the&nbsp;`state_dict`.&nbsp;Implement&nbsp;this&nbsp;function&nbsp;and&nbsp;a&nbsp;corresponding<br>
:func:`get_extra_state`&nbsp;for&nbsp;your&nbsp;module&nbsp;if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state&nbsp;within&nbsp;its<br>
`state_dict`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;(dict):&nbsp;Extra&nbsp;state&nbsp;from&nbsp;the&nbsp;`state_dict`</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-set_submodule"><strong>set_submodule</strong></a>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</dt><dd><span class="code">Set&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``False``&nbsp;(default),&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error&nbsp;if<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;submodule&nbsp;does&nbsp;not&nbsp;exist.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(3,&nbsp;3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;override&nbsp;the&nbsp;``Conv2d``&nbsp;with&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Linear``,&nbsp;you<br>
could&nbsp;call&nbsp;``<a href="#MonteCarloDropoutEstimator-set_submodule">set_submodule</a>("net_b.net_c.conv",&nbsp;nn.Linear(1,&nbsp;1))``<br>
where&nbsp;``strict``&nbsp;could&nbsp;be&nbsp;``True``&nbsp;or&nbsp;``False``<br>
&nbsp;<br>
To&nbsp;add&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Conv2d``&nbsp;to&nbsp;the&nbsp;existing&nbsp;``net_b``&nbsp;module,<br>
you&nbsp;would&nbsp;call&nbsp;``<a href="#MonteCarloDropoutEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1))``.<br>
&nbsp;<br>
In&nbsp;the&nbsp;above&nbsp;if&nbsp;you&nbsp;set&nbsp;``strict=True``&nbsp;and&nbsp;call<br>
``<a href="#MonteCarloDropoutEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1),&nbsp;strict=True)``,&nbsp;an&nbsp;AttributeError<br>
will&nbsp;be&nbsp;raised&nbsp;because&nbsp;``net_b``&nbsp;does&nbsp;not&nbsp;have&nbsp;a&nbsp;submodule&nbsp;named&nbsp;``conv``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;module:&nbsp;The&nbsp;module&nbsp;to&nbsp;set&nbsp;the&nbsp;submodule&nbsp;to.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;If&nbsp;``False``,&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;the&nbsp;submodule&nbsp;doesn't&nbsp;already&nbsp;exist.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;``target``&nbsp;string&nbsp;is&nbsp;empty&nbsp;or&nbsp;if&nbsp;``module``&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;``target``&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-share_memory"><strong>share_memory</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">See&nbsp;:meth:`torch.Tensor.share_memory_`.</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-state_dict"><strong>state_dict</strong></a>(self, *args, destination=None, prefix='', keep_vars=False)</dt><dd><span class="code">Return&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;references&nbsp;to&nbsp;the&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
Parameters&nbsp;and&nbsp;buffers&nbsp;set&nbsp;to&nbsp;``None``&nbsp;are&nbsp;not&nbsp;included.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;returned&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;a&nbsp;shallow&nbsp;copy.&nbsp;It&nbsp;contains&nbsp;references<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;module's&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;``<a href="#MonteCarloDropoutEstimator-state_dict">state_dict</a>()``&nbsp;also&nbsp;accepts&nbsp;positional&nbsp;arguments&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;``destination``,&nbsp;``prefix``&nbsp;and&nbsp;``keep_vars``&nbsp;in&nbsp;order.&nbsp;However,<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;being&nbsp;deprecated&nbsp;and&nbsp;keyword&nbsp;arguments&nbsp;will&nbsp;be&nbsp;enforced&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;future&nbsp;releases.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;avoid&nbsp;the&nbsp;use&nbsp;of&nbsp;argument&nbsp;``destination``&nbsp;as&nbsp;it&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;designed&nbsp;for&nbsp;end-users.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;destination&nbsp;(dict,&nbsp;optional):&nbsp;If&nbsp;provided,&nbsp;the&nbsp;state&nbsp;of&nbsp;module&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;updated&nbsp;into&nbsp;the&nbsp;dict&nbsp;and&nbsp;the&nbsp;same&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;an&nbsp;``OrderedDict``&nbsp;will&nbsp;be&nbsp;created&nbsp;and&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``None``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str,&nbsp;optional):&nbsp;a&nbsp;prefix&nbsp;added&nbsp;to&nbsp;parameter&nbsp;and&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;names&nbsp;to&nbsp;compose&nbsp;the&nbsp;keys&nbsp;in&nbsp;state_dict.&nbsp;Default:&nbsp;``''``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;keep_vars&nbsp;(bool,&nbsp;optional):&nbsp;by&nbsp;default&nbsp;the&nbsp;:class:`~torch.Tensor`&nbsp;s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returned&nbsp;in&nbsp;the&nbsp;state&nbsp;dict&nbsp;are&nbsp;detached&nbsp;from&nbsp;autograd.&nbsp;If&nbsp;it's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;to&nbsp;``True``,&nbsp;detaching&nbsp;will&nbsp;not&nbsp;be&nbsp;performed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#MonteCarloDropoutEstimator-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Move&nbsp;and/or&nbsp;cast&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MonteCarloDropoutEstimator-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MonteCarloDropoutEstimator-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MonteCarloDropoutEstimator-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MonteCarloDropoutEstimator-to">to</a>(memory_format=torch.channels_last)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;:attr:`dtype`\&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;dtype&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_format&nbsp;(:class:`torch.memory_format`):&nbsp;the&nbsp;desired&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format&nbsp;for&nbsp;4D&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module&nbsp;(keyword<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;argument)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+IGNORE_WANT("non-deterministic")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MonteCarloDropoutEstimator-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+REQUIRES(env:TORCH_DOCTEST_CUDA1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MonteCarloDropoutEstimator-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MonteCarloDropoutEstimator-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2,&nbsp;bias=None).<a href="#MonteCarloDropoutEstimator-to">to</a>(torch.cdouble)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.3741+0.j,&nbsp;&nbsp;0.2382+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;0.5593+0.j,&nbsp;-0.4443+0.j]],&nbsp;dtype=torch.complex128)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear(torch.ones(3,&nbsp;2,&nbsp;dtype=torch.cdouble))<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j]],&nbsp;dtype=torch.complex128)</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-to_empty"><strong>to_empty</strong></a>(
    self: ~T,
    *,
    device: Union[int, str, torch.device, NoneType],
    recurse: bool = True
) -&gt; ~T</dt><dd><span class="code">Move&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;specified&nbsp;device&nbsp;without&nbsp;copying&nbsp;storage.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;The&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;Whether&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;of&nbsp;submodules&nbsp;should<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;recursively&nbsp;moved&nbsp;to&nbsp;the&nbsp;specified&nbsp;device.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-train"><strong>train</strong></a>(self: ~T, mode: bool = True) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.,&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-type"><strong>type</strong></a>(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-xpu"><strong>xpu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;XPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;XPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="MonteCarloDropoutEstimator-zero_grad"><strong>zero_grad</strong></a>(self, set_to_none: bool = True) -&gt; None</dt><dd><span class="code">Reset&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters.<br>
&nbsp;<br>
See&nbsp;similar&nbsp;function&nbsp;under&nbsp;:class:`torch.optim.Optimizer`&nbsp;for&nbsp;more&nbsp;context.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;set_to_none&nbsp;(bool):&nbsp;instead&nbsp;of&nbsp;setting&nbsp;to&nbsp;zero,&nbsp;set&nbsp;the&nbsp;grads&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;:meth:`torch.optim.Optimizer.zero_grad`&nbsp;for&nbsp;details.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>T_destination</strong> = ~T_destination</dl>

<dl><dt><strong>call_super_init</strong> = False</dl>

<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="SymbolicConsistencyEstimator">class <strong>SymbolicConsistencyEstimator</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#SymbolicConsistencyEstimator">SymbolicConsistencyEstimator</a>(symbolic_dim:&nbsp;int&nbsp;=&nbsp;512)<br>
&nbsp;<br>
Confidence&nbsp;estimation&nbsp;based&nbsp;on&nbsp;symbolic&nbsp;reasoning&nbsp;consistency<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="adapters.confidence_estimator.html#SymbolicConsistencyEstimator">SymbolicConsistencyEstimator</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="SymbolicConsistencyEstimator-__init__"><strong>__init__</strong></a>(self, symbolic_dim: int = 512)</dt><dd><span class="code">Initialize&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-forward"><strong>forward</strong></a>(
    self,
    symbolic_embeddings: torch.Tensor,
    extracted_facts: Optional[List[List[str]]] = None
) -&gt; torch.Tensor</dt><dd><span class="code">Estimate&nbsp;confidence&nbsp;based&nbsp;on&nbsp;symbolic&nbsp;consistency<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;symbolic_embeddings:&nbsp;Symbolic&nbsp;representations&nbsp;[batch_size,&nbsp;seq_len,&nbsp;symbolic_dim]<br>
&nbsp;&nbsp;&nbsp;&nbsp;extracted_facts:&nbsp;Extracted&nbsp;symbolic&nbsp;facts&nbsp;(optional)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Confidence&nbsp;scores&nbsp;[batch_size,&nbsp;seq_len]</span></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__annotations__</strong> = {}</dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="SymbolicConsistencyEstimator-__call__"><strong>__call__</strong></a> = <a href="#SymbolicConsistencyEstimator-_wrapped_call_impl">_wrapped_call_impl</a>(self, *args, **kwargs)</dt></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><span class="code">Implement&nbsp;delattr(self,&nbsp;name).</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><span class="code">Default&nbsp;dir()&nbsp;implementation.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-__getattr__"><strong>__getattr__</strong></a>(self, name: str) -&gt; Union[torch.Tensor, ForwardRef('Module')]</dt><dd><span class="code">#&nbsp;It&nbsp;is&nbsp;crucial&nbsp;that&nbsp;the&nbsp;return&nbsp;type&nbsp;is&nbsp;not&nbsp;annotated&nbsp;as&nbsp;`Any`,&nbsp;otherwise&nbsp;type&nbsp;checking<br>
#&nbsp;on&nbsp;`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;and&nbsp;all&nbsp;its&nbsp;subclasses&nbsp;is&nbsp;largely&nbsp;disabled&nbsp;as&nbsp;a&nbsp;result.&nbsp;See:<br>
#&nbsp;<a href="https://github.com/pytorch/pytorch/pull/115074">https://github.com/pytorch/pytorch/pull/115074</a></span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-__getstate__"><strong>__getstate__</strong></a>(self)</dt><dd><span class="code">Helper&nbsp;for&nbsp;pickle.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-__setattr__"><strong>__setattr__</strong></a>(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-add_module"><strong>add_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-apply"><strong>apply</strong></a>(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</dt><dd><span class="code">Apply&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#SymbolicConsistencyEstimator-children">children</a>()``)&nbsp;as&nbsp;well&nbsp;as&nbsp;self.<br>
&nbsp;<br>
Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;@torch.no_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#SymbolicConsistencyEstimator-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#SymbolicConsistencyEstimator-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-bfloat16"><strong>bfloat16</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``bfloat16``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-buffers"><strong>buffers</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#SymbolicConsistencyEstimator-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#SymbolicConsistencyEstimator-type">type</a>(buf),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-children"><strong>children</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-compile"><strong>compile</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Compile&nbsp;this&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;using&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
This&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;`__call__`&nbsp;method&nbsp;is&nbsp;compiled&nbsp;and&nbsp;all&nbsp;arguments&nbsp;are&nbsp;passed&nbsp;as-is<br>
to&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
See&nbsp;:func:`torch.compile`&nbsp;for&nbsp;details&nbsp;on&nbsp;the&nbsp;arguments&nbsp;for&nbsp;this&nbsp;function.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-cpu"><strong>cpu</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-cuda"><strong>cuda</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-double"><strong>double</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-eval"><strong>eval</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#SymbolicConsistencyEstimator-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#SymbolicConsistencyEstimator-eval">eval</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-extra_repr"><strong>extra_repr</strong></a>(self) -&gt; str</dt><dd><span class="code">Return&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;re-implement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-float"><strong>float</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``float``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-get_buffer"><strong>get_buffer</strong></a>(self, target: str) -&gt; 'Tensor'</dt><dd><span class="code">Return&nbsp;the&nbsp;buffer&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;The&nbsp;buffer&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-get_extra_state"><strong>get_extra_state</strong></a>(self) -&gt; Any</dt><dd><span class="code">Return&nbsp;any&nbsp;extra&nbsp;state&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict.<br>
&nbsp;<br>
Implement&nbsp;this&nbsp;and&nbsp;a&nbsp;corresponding&nbsp;:func:`set_extra_state`&nbsp;for&nbsp;your&nbsp;module<br>
if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state.&nbsp;This&nbsp;function&nbsp;is&nbsp;called&nbsp;when&nbsp;building&nbsp;the<br>
module's&nbsp;`<a href="#SymbolicConsistencyEstimator-state_dict">state_dict</a>()`.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;extra&nbsp;state&nbsp;should&nbsp;be&nbsp;picklable&nbsp;to&nbsp;ensure&nbsp;working&nbsp;serialization<br>
of&nbsp;the&nbsp;state_dict.&nbsp;We&nbsp;only&nbsp;provide&nbsp;backwards&nbsp;compatibility&nbsp;guarantees<br>
for&nbsp;serializing&nbsp;Tensors;&nbsp;other&nbsp;objects&nbsp;may&nbsp;break&nbsp;backwards&nbsp;compatibility&nbsp;if<br>
their&nbsp;serialized&nbsp;pickled&nbsp;form&nbsp;changes.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="builtins.html#object">object</a>:&nbsp;Any&nbsp;extra&nbsp;state&nbsp;to&nbsp;store&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-get_parameter"><strong>get_parameter</strong></a>(self, target: str) -&gt; 'Parameter'</dt><dd><span class="code">Return&nbsp;the&nbsp;parameter&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;Parameter<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Parameter:&nbsp;The&nbsp;Parameter&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``nn.Parameter``</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-get_submodule"><strong>get_submodule</strong></a>(self, target: str) -&gt; 'Module'</dt><dd><span class="code">Return&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(16,&nbsp;33,&nbsp;kernel_size=(3,&nbsp;3),&nbsp;stride=(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(in_features=100,&nbsp;out_features=200,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;which&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;check&nbsp;whether&nbsp;or&nbsp;not&nbsp;we&nbsp;have&nbsp;the&nbsp;``linear``&nbsp;submodule,&nbsp;we<br>
would&nbsp;call&nbsp;``<a href="#SymbolicConsistencyEstimator-get_submodule">get_submodule</a>("net_b.linear")``.&nbsp;To&nbsp;check&nbsp;whether<br>
we&nbsp;have&nbsp;the&nbsp;``conv``&nbsp;submodule,&nbsp;we&nbsp;would&nbsp;call<br>
``<a href="#SymbolicConsistencyEstimator-get_submodule">get_submodule</a>("net_b.net_c.conv")``.<br>
&nbsp;<br>
The&nbsp;runtime&nbsp;of&nbsp;``get_submodule``&nbsp;is&nbsp;bounded&nbsp;by&nbsp;the&nbsp;degree<br>
of&nbsp;module&nbsp;nesting&nbsp;in&nbsp;``target``.&nbsp;A&nbsp;query&nbsp;against<br>
``named_modules``&nbsp;achieves&nbsp;the&nbsp;same&nbsp;result,&nbsp;but&nbsp;it&nbsp;is&nbsp;O(N)&nbsp;in<br>
the&nbsp;number&nbsp;of&nbsp;transitive&nbsp;modules.&nbsp;So,&nbsp;for&nbsp;a&nbsp;simple&nbsp;check&nbsp;to&nbsp;see<br>
if&nbsp;some&nbsp;submodule&nbsp;exists,&nbsp;``get_submodule``&nbsp;should&nbsp;always&nbsp;be<br>
used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;The&nbsp;submodule&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-half"><strong>half</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-ipu"><strong>ipu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;IPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;IPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-load_state_dict"><strong>load_state_dict</strong></a>(
    self,
    state_dict: collections.abc.Mapping[str, typing.Any],
    strict: bool = True,
    assign: bool = False
)</dt><dd><span class="code">Copy&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into&nbsp;this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.<br>
&nbsp;<br>
If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;:attr:`assign`&nbsp;is&nbsp;``True``&nbsp;the&nbsp;optimizer&nbsp;must&nbsp;be&nbsp;created&nbsp;after<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;call&nbsp;to&nbsp;:attr:`load_state_dict`&nbsp;unless<br>
&nbsp;&nbsp;&nbsp;&nbsp;:func:`~torch.__future__.get_swap_module_params_on_conversion`&nbsp;is&nbsp;``True``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;&nbsp;&nbsp;&nbsp;assign&nbsp;(bool,&nbsp;optional):&nbsp;When&nbsp;set&nbsp;to&nbsp;``False``,&nbsp;the&nbsp;properties&nbsp;of&nbsp;the&nbsp;tensors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;current&nbsp;module&nbsp;are&nbsp;preserved&nbsp;whereas&nbsp;setting&nbsp;it&nbsp;to&nbsp;``True``&nbsp;preserves<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;properties&nbsp;of&nbsp;the&nbsp;Tensors&nbsp;in&nbsp;the&nbsp;state&nbsp;dict.&nbsp;The&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exception&nbsp;is&nbsp;the&nbsp;``requires_grad``&nbsp;field&nbsp;of&nbsp;:class:`~torch.nn.Parameter`s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;which&nbsp;the&nbsp;value&nbsp;from&nbsp;the&nbsp;module&nbsp;is&nbsp;preserved.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;any&nbsp;keys&nbsp;that&nbsp;are&nbsp;expected<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;missing&nbsp;from&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;keys&nbsp;that&nbsp;are&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expected&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;present&nbsp;in&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;a&nbsp;parameter&nbsp;or&nbsp;buffer&nbsp;is&nbsp;registered&nbsp;as&nbsp;``None``&nbsp;and&nbsp;its&nbsp;corresponding&nbsp;key<br>
&nbsp;&nbsp;&nbsp;&nbsp;exists&nbsp;in&nbsp;:attr:`state_dict`,&nbsp;:meth:`load_state_dict`&nbsp;will&nbsp;raise&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;``RuntimeError``.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-modules"><strong>modules</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#SymbolicConsistencyEstimator-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-mtia"><strong>mtia</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;MTIA.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;MTIA&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-named_buffers"><strong>named_buffers</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool,&nbsp;optional):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;buffers&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#SymbolicConsistencyEstimator-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-named_children"><strong>named_children</strong></a>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#SymbolicConsistencyEstimator-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-named_modules"><strong>named_modules</strong></a>(
    self,
    memo: Optional[set['Module']] = None,
    prefix: str = '',
    remove_duplicate: bool = True
)</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;memo:&nbsp;a&nbsp;memo&nbsp;to&nbsp;store&nbsp;the&nbsp;set&nbsp;of&nbsp;modules&nbsp;already&nbsp;added&nbsp;to&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix:&nbsp;a&nbsp;prefix&nbsp;that&nbsp;will&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate:&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;module&nbsp;instances&nbsp;in&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;not<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#SymbolicConsistencyEstimator-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-named_parameters"><strong>named_parameters</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#SymbolicConsistencyEstimator-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-parameters"><strong>parameters</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#SymbolicConsistencyEstimator-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#SymbolicConsistencyEstimator-type">type</a>(param),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_backward_hook"><strong>register_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;deprecated&nbsp;in&nbsp;favor&nbsp;of&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.register_full_backward_hook`&nbsp;and<br>
the&nbsp;behavior&nbsp;of&nbsp;this&nbsp;function&nbsp;will&nbsp;change&nbsp;in&nbsp;future&nbsp;versions.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_buffer"><strong>register_buffer</strong></a>(
    self,
    name: str,
    tensor: Optional[torch.Tensor],
    persistent: bool = True
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;module's&nbsp;state.&nbsp;Buffers,&nbsp;by<br>
default,&nbsp;are&nbsp;persistent&nbsp;and&nbsp;will&nbsp;be&nbsp;saved&nbsp;alongside&nbsp;parameters.&nbsp;This<br>
behavior&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;setting&nbsp;:attr:`persistent`&nbsp;to&nbsp;``False``.&nbsp;The<br>
only&nbsp;difference&nbsp;between&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;and&nbsp;a&nbsp;non-persistent&nbsp;buffer<br>
is&nbsp;that&nbsp;the&nbsp;latter&nbsp;will&nbsp;not&nbsp;be&nbsp;a&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
:attr:`state_dict`.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor&nbsp;or&nbsp;None):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.&nbsp;If&nbsp;``None``,&nbsp;then&nbsp;operations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;run&nbsp;on&nbsp;buffers,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;buffer&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the&nbsp;module's&nbsp;:attr:`state_dict`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;(bool):&nbsp;whether&nbsp;the&nbsp;buffer&nbsp;is&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:attr:`state_dict`.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#SymbolicConsistencyEstimator-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_forward_hook"><strong>register_forward_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False,
    always_call: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``False``&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but&nbsp;it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on<br>
forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after&nbsp;:func:`forward`&nbsp;is&nbsp;called.&nbsp;The&nbsp;hook<br>
should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``True``,&nbsp;the&nbsp;forward&nbsp;hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
``kwargs``&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function&nbsp;and&nbsp;be&nbsp;expected&nbsp;to&nbsp;return&nbsp;the<br>
output&nbsp;possibly&nbsp;modified.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;always_call&nbsp;(bool):&nbsp;If&nbsp;``True``&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;run&nbsp;regardless&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;whether&nbsp;an&nbsp;exception&nbsp;is&nbsp;raised&nbsp;while&nbsp;calling&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
&nbsp;<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;false&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a&nbsp;single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the<br>
hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple&nbsp;if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned<br>
(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the<br>
following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;true,&nbsp;the&nbsp;forward&nbsp;pre-hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.&nbsp;And&nbsp;if&nbsp;the&nbsp;hook&nbsp;modifies&nbsp;the<br>
input,&nbsp;both&nbsp;the&nbsp;args&nbsp;and&nbsp;kwargs&nbsp;should&nbsp;be&nbsp;returned.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have<br>
the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;a&nbsp;tuple&nbsp;of&nbsp;modified&nbsp;input&nbsp;and&nbsp;kwargs<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the&nbsp;kwargs<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_full_backward_hook"><strong>register_full_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;a&nbsp;module<br>
are&nbsp;computed,&nbsp;i.e.&nbsp;the&nbsp;hook&nbsp;will&nbsp;execute&nbsp;if&nbsp;and&nbsp;only&nbsp;if&nbsp;the&nbsp;gradients&nbsp;with<br>
respect&nbsp;to&nbsp;module&nbsp;outputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following<br>
signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple(Tensor)&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;are&nbsp;tuples&nbsp;that&nbsp;contain&nbsp;the&nbsp;gradients<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs&nbsp;respectively.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;:attr:`grad_input`&nbsp;will&nbsp;only&nbsp;correspond&nbsp;to&nbsp;the&nbsp;inputs&nbsp;given<br>
as&nbsp;positional&nbsp;arguments&nbsp;and&nbsp;all&nbsp;kwarg&nbsp;arguments&nbsp;are&nbsp;ignored.&nbsp;Entries<br>
in&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for&nbsp;all&nbsp;non-Tensor<br>
arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;or&nbsp;outputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_full_backward_pre_hook"><strong>register_full_backward_pre_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;for&nbsp;the&nbsp;module&nbsp;are&nbsp;computed.<br>
The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple[Tensor]&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_output`&nbsp;is&nbsp;a&nbsp;tuple.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;output&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_output`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;Entries&nbsp;in&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for<br>
all&nbsp;non-Tensor&nbsp;arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_load_state_dict_post_hook"><strong>register_load_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;after&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;incompatible_keys)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;``module``&nbsp;argument&nbsp;is&nbsp;the&nbsp;current&nbsp;module&nbsp;that&nbsp;this&nbsp;hook&nbsp;is&nbsp;registered<br>
on,&nbsp;and&nbsp;the&nbsp;``incompatible_keys``&nbsp;argument&nbsp;is&nbsp;a&nbsp;``NamedTuple``&nbsp;consisting<br>
of&nbsp;attributes&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``.&nbsp;``missing_keys``<br>
is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys&nbsp;and<br>
``unexpected_keys``&nbsp;is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys.<br>
&nbsp;<br>
The&nbsp;given&nbsp;incompatible_keys&nbsp;can&nbsp;be&nbsp;modified&nbsp;inplace&nbsp;if&nbsp;needed.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;checks&nbsp;performed&nbsp;when&nbsp;calling&nbsp;:func:`load_state_dict`&nbsp;with<br>
``strict=True``&nbsp;are&nbsp;affected&nbsp;by&nbsp;modifications&nbsp;the&nbsp;hook&nbsp;makes&nbsp;to<br>
``missing_keys``&nbsp;or&nbsp;``unexpected_keys``,&nbsp;as&nbsp;expected.&nbsp;Additions&nbsp;to&nbsp;either<br>
set&nbsp;of&nbsp;keys&nbsp;will&nbsp;result&nbsp;in&nbsp;an&nbsp;error&nbsp;being&nbsp;thrown&nbsp;when&nbsp;``strict=True``,&nbsp;and<br>
clearing&nbsp;out&nbsp;both&nbsp;missing&nbsp;and&nbsp;unexpected&nbsp;keys&nbsp;will&nbsp;avoid&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_load_state_dict_pre_hook"><strong>register_load_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;before&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata,&nbsp;strict,&nbsp;missing_keys,&nbsp;unexpected_keys,&nbsp;error_msgs)&nbsp;-&gt;&nbsp;None&nbsp;&nbsp;#&nbsp;noqa:&nbsp;B950<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;Callable&nbsp;hook&nbsp;that&nbsp;will&nbsp;be&nbsp;invoked&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loading&nbsp;the&nbsp;state&nbsp;dict.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_module"><strong>register_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Alias&nbsp;for&nbsp;:func:`add_module`.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_parameter"><strong>register_parameter</strong></a>(
    self,
    name: str,
    param: Optional[torch.nn.parameter.Parameter]
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter&nbsp;or&nbsp;None):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``None``,&nbsp;then&nbsp;operations&nbsp;that&nbsp;run&nbsp;on&nbsp;parameters,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,&nbsp;the&nbsp;parameter&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;module's&nbsp;:attr:`state_dict`.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_state_dict_post_hook"><strong>register_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;modify&nbsp;the&nbsp;``state_dict``&nbsp;inplace.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-register_state_dict_pre_hook"><strong>register_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;prefix,&nbsp;keep_vars)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;perform&nbsp;pre-processing&nbsp;before&nbsp;the&nbsp;``state_dict``<br>
call&nbsp;is&nbsp;made.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-requires_grad_"><strong>requires_grad_</strong></a>(self: ~T, requires_grad: bool = True) -&gt; ~T</dt><dd><span class="code">Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#SymbolicConsistencyEstimator-requires_grad_">requires_grad_</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-set_extra_state"><strong>set_extra_state</strong></a>(self, state: Any) -&gt; None</dt><dd><span class="code">Set&nbsp;extra&nbsp;state&nbsp;contained&nbsp;in&nbsp;the&nbsp;loaded&nbsp;`state_dict`.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;called&nbsp;from&nbsp;:func:`load_state_dict`&nbsp;to&nbsp;handle&nbsp;any&nbsp;extra&nbsp;state<br>
found&nbsp;within&nbsp;the&nbsp;`state_dict`.&nbsp;Implement&nbsp;this&nbsp;function&nbsp;and&nbsp;a&nbsp;corresponding<br>
:func:`get_extra_state`&nbsp;for&nbsp;your&nbsp;module&nbsp;if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state&nbsp;within&nbsp;its<br>
`state_dict`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;(dict):&nbsp;Extra&nbsp;state&nbsp;from&nbsp;the&nbsp;`state_dict`</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-set_submodule"><strong>set_submodule</strong></a>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</dt><dd><span class="code">Set&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``False``&nbsp;(default),&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error&nbsp;if<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;submodule&nbsp;does&nbsp;not&nbsp;exist.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(3,&nbsp;3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;override&nbsp;the&nbsp;``Conv2d``&nbsp;with&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Linear``,&nbsp;you<br>
could&nbsp;call&nbsp;``<a href="#SymbolicConsistencyEstimator-set_submodule">set_submodule</a>("net_b.net_c.conv",&nbsp;nn.Linear(1,&nbsp;1))``<br>
where&nbsp;``strict``&nbsp;could&nbsp;be&nbsp;``True``&nbsp;or&nbsp;``False``<br>
&nbsp;<br>
To&nbsp;add&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Conv2d``&nbsp;to&nbsp;the&nbsp;existing&nbsp;``net_b``&nbsp;module,<br>
you&nbsp;would&nbsp;call&nbsp;``<a href="#SymbolicConsistencyEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1))``.<br>
&nbsp;<br>
In&nbsp;the&nbsp;above&nbsp;if&nbsp;you&nbsp;set&nbsp;``strict=True``&nbsp;and&nbsp;call<br>
``<a href="#SymbolicConsistencyEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1),&nbsp;strict=True)``,&nbsp;an&nbsp;AttributeError<br>
will&nbsp;be&nbsp;raised&nbsp;because&nbsp;``net_b``&nbsp;does&nbsp;not&nbsp;have&nbsp;a&nbsp;submodule&nbsp;named&nbsp;``conv``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;module:&nbsp;The&nbsp;module&nbsp;to&nbsp;set&nbsp;the&nbsp;submodule&nbsp;to.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;If&nbsp;``False``,&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;the&nbsp;submodule&nbsp;doesn't&nbsp;already&nbsp;exist.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;``target``&nbsp;string&nbsp;is&nbsp;empty&nbsp;or&nbsp;if&nbsp;``module``&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;``target``&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-share_memory"><strong>share_memory</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">See&nbsp;:meth:`torch.Tensor.share_memory_`.</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-state_dict"><strong>state_dict</strong></a>(self, *args, destination=None, prefix='', keep_vars=False)</dt><dd><span class="code">Return&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;references&nbsp;to&nbsp;the&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
Parameters&nbsp;and&nbsp;buffers&nbsp;set&nbsp;to&nbsp;``None``&nbsp;are&nbsp;not&nbsp;included.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;returned&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;a&nbsp;shallow&nbsp;copy.&nbsp;It&nbsp;contains&nbsp;references<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;module's&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;``<a href="#SymbolicConsistencyEstimator-state_dict">state_dict</a>()``&nbsp;also&nbsp;accepts&nbsp;positional&nbsp;arguments&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;``destination``,&nbsp;``prefix``&nbsp;and&nbsp;``keep_vars``&nbsp;in&nbsp;order.&nbsp;However,<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;being&nbsp;deprecated&nbsp;and&nbsp;keyword&nbsp;arguments&nbsp;will&nbsp;be&nbsp;enforced&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;future&nbsp;releases.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;avoid&nbsp;the&nbsp;use&nbsp;of&nbsp;argument&nbsp;``destination``&nbsp;as&nbsp;it&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;designed&nbsp;for&nbsp;end-users.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;destination&nbsp;(dict,&nbsp;optional):&nbsp;If&nbsp;provided,&nbsp;the&nbsp;state&nbsp;of&nbsp;module&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;updated&nbsp;into&nbsp;the&nbsp;dict&nbsp;and&nbsp;the&nbsp;same&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;an&nbsp;``OrderedDict``&nbsp;will&nbsp;be&nbsp;created&nbsp;and&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``None``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str,&nbsp;optional):&nbsp;a&nbsp;prefix&nbsp;added&nbsp;to&nbsp;parameter&nbsp;and&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;names&nbsp;to&nbsp;compose&nbsp;the&nbsp;keys&nbsp;in&nbsp;state_dict.&nbsp;Default:&nbsp;``''``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;keep_vars&nbsp;(bool,&nbsp;optional):&nbsp;by&nbsp;default&nbsp;the&nbsp;:class:`~torch.Tensor`&nbsp;s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returned&nbsp;in&nbsp;the&nbsp;state&nbsp;dict&nbsp;are&nbsp;detached&nbsp;from&nbsp;autograd.&nbsp;If&nbsp;it's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;to&nbsp;``True``,&nbsp;detaching&nbsp;will&nbsp;not&nbsp;be&nbsp;performed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#SymbolicConsistencyEstimator-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Move&nbsp;and/or&nbsp;cast&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#SymbolicConsistencyEstimator-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#SymbolicConsistencyEstimator-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#SymbolicConsistencyEstimator-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#SymbolicConsistencyEstimator-to">to</a>(memory_format=torch.channels_last)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;:attr:`dtype`\&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;dtype&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_format&nbsp;(:class:`torch.memory_format`):&nbsp;the&nbsp;desired&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format&nbsp;for&nbsp;4D&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module&nbsp;(keyword<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;argument)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+IGNORE_WANT("non-deterministic")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#SymbolicConsistencyEstimator-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+REQUIRES(env:TORCH_DOCTEST_CUDA1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#SymbolicConsistencyEstimator-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#SymbolicConsistencyEstimator-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2,&nbsp;bias=None).<a href="#SymbolicConsistencyEstimator-to">to</a>(torch.cdouble)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.3741+0.j,&nbsp;&nbsp;0.2382+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;0.5593+0.j,&nbsp;-0.4443+0.j]],&nbsp;dtype=torch.complex128)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear(torch.ones(3,&nbsp;2,&nbsp;dtype=torch.cdouble))<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j]],&nbsp;dtype=torch.complex128)</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-to_empty"><strong>to_empty</strong></a>(
    self: ~T,
    *,
    device: Union[int, str, torch.device, NoneType],
    recurse: bool = True
) -&gt; ~T</dt><dd><span class="code">Move&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;specified&nbsp;device&nbsp;without&nbsp;copying&nbsp;storage.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;The&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;Whether&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;of&nbsp;submodules&nbsp;should<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;recursively&nbsp;moved&nbsp;to&nbsp;the&nbsp;specified&nbsp;device.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-train"><strong>train</strong></a>(self: ~T, mode: bool = True) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.,&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-type"><strong>type</strong></a>(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-xpu"><strong>xpu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;XPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;XPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="SymbolicConsistencyEstimator-zero_grad"><strong>zero_grad</strong></a>(self, set_to_none: bool = True) -&gt; None</dt><dd><span class="code">Reset&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters.<br>
&nbsp;<br>
See&nbsp;similar&nbsp;function&nbsp;under&nbsp;:class:`torch.optim.Optimizer`&nbsp;for&nbsp;more&nbsp;context.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;set_to_none&nbsp;(bool):&nbsp;instead&nbsp;of&nbsp;setting&nbsp;to&nbsp;zero,&nbsp;set&nbsp;the&nbsp;grads&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;:meth:`torch.optim.Optimizer.zero_grad`&nbsp;for&nbsp;details.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>T_destination</strong> = ~T_destination</dl>

<dl><dt><strong>call_super_init</strong> = False</dl>

<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="TemperatureScalingEstimator">class <strong>TemperatureScalingEstimator</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#TemperatureScalingEstimator">TemperatureScalingEstimator</a>(initial_temperature:&nbsp;float&nbsp;=&nbsp;1.0)<br>
&nbsp;<br>
Temperature&nbsp;scaling&nbsp;for&nbsp;calibrated&nbsp;confidence&nbsp;estimation<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="adapters.confidence_estimator.html#TemperatureScalingEstimator">TemperatureScalingEstimator</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="TemperatureScalingEstimator-__init__"><strong>__init__</strong></a>(self, initial_temperature: float = 1.0)</dt><dd><span class="code">Initialize&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-forward"><strong>forward</strong></a>(self, logits: torch.Tensor) -&gt; torch.Tensor</dt><dd><span class="code">Apply&nbsp;temperature&nbsp;scaling&nbsp;to&nbsp;logits<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits:&nbsp;Model&nbsp;logits&nbsp;[batch_size,&nbsp;seq_len,&nbsp;vocab_size]<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Confidence&nbsp;scores&nbsp;[batch_size,&nbsp;seq_len]</span></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__annotations__</strong> = {}</dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="TemperatureScalingEstimator-__call__"><strong>__call__</strong></a> = <a href="#TemperatureScalingEstimator-_wrapped_call_impl">_wrapped_call_impl</a>(self, *args, **kwargs)</dt></dl>

<dl><dt><a name="TemperatureScalingEstimator-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><span class="code">Implement&nbsp;delattr(self,&nbsp;name).</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><span class="code">Default&nbsp;dir()&nbsp;implementation.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-__getattr__"><strong>__getattr__</strong></a>(self, name: str) -&gt; Union[torch.Tensor, ForwardRef('Module')]</dt><dd><span class="code">#&nbsp;It&nbsp;is&nbsp;crucial&nbsp;that&nbsp;the&nbsp;return&nbsp;type&nbsp;is&nbsp;not&nbsp;annotated&nbsp;as&nbsp;`Any`,&nbsp;otherwise&nbsp;type&nbsp;checking<br>
#&nbsp;on&nbsp;`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;and&nbsp;all&nbsp;its&nbsp;subclasses&nbsp;is&nbsp;largely&nbsp;disabled&nbsp;as&nbsp;a&nbsp;result.&nbsp;See:<br>
#&nbsp;<a href="https://github.com/pytorch/pytorch/pull/115074">https://github.com/pytorch/pytorch/pull/115074</a></span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-__getstate__"><strong>__getstate__</strong></a>(self)</dt><dd><span class="code">Helper&nbsp;for&nbsp;pickle.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-__setattr__"><strong>__setattr__</strong></a>(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="TemperatureScalingEstimator-add_module"><strong>add_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-apply"><strong>apply</strong></a>(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</dt><dd><span class="code">Apply&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#TemperatureScalingEstimator-children">children</a>()``)&nbsp;as&nbsp;well&nbsp;as&nbsp;self.<br>
&nbsp;<br>
Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;@torch.no_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#TemperatureScalingEstimator-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#TemperatureScalingEstimator-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[1.,&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.,&nbsp;1.]],&nbsp;requires_grad=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-bfloat16"><strong>bfloat16</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``bfloat16``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-buffers"><strong>buffers</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#TemperatureScalingEstimator-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#TemperatureScalingEstimator-type">type</a>(buf),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-children"><strong>children</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-compile"><strong>compile</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Compile&nbsp;this&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;using&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
This&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;`__call__`&nbsp;method&nbsp;is&nbsp;compiled&nbsp;and&nbsp;all&nbsp;arguments&nbsp;are&nbsp;passed&nbsp;as-is<br>
to&nbsp;:func:`torch.compile`.<br>
&nbsp;<br>
See&nbsp;:func:`torch.compile`&nbsp;for&nbsp;details&nbsp;on&nbsp;the&nbsp;arguments&nbsp;for&nbsp;this&nbsp;function.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-cpu"><strong>cpu</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-cuda"><strong>cuda</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-double"><strong>double</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-eval"><strong>eval</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#TemperatureScalingEstimator-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#TemperatureScalingEstimator-eval">eval</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-extra_repr"><strong>extra_repr</strong></a>(self) -&gt; str</dt><dd><span class="code">Return&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;re-implement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-float"><strong>float</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``float``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-get_buffer"><strong>get_buffer</strong></a>(self, target: str) -&gt; 'Tensor'</dt><dd><span class="code">Return&nbsp;the&nbsp;buffer&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;The&nbsp;buffer&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-get_extra_state"><strong>get_extra_state</strong></a>(self) -&gt; Any</dt><dd><span class="code">Return&nbsp;any&nbsp;extra&nbsp;state&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict.<br>
&nbsp;<br>
Implement&nbsp;this&nbsp;and&nbsp;a&nbsp;corresponding&nbsp;:func:`set_extra_state`&nbsp;for&nbsp;your&nbsp;module<br>
if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state.&nbsp;This&nbsp;function&nbsp;is&nbsp;called&nbsp;when&nbsp;building&nbsp;the<br>
module's&nbsp;`<a href="#TemperatureScalingEstimator-state_dict">state_dict</a>()`.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;extra&nbsp;state&nbsp;should&nbsp;be&nbsp;picklable&nbsp;to&nbsp;ensure&nbsp;working&nbsp;serialization<br>
of&nbsp;the&nbsp;state_dict.&nbsp;We&nbsp;only&nbsp;provide&nbsp;backwards&nbsp;compatibility&nbsp;guarantees<br>
for&nbsp;serializing&nbsp;Tensors;&nbsp;other&nbsp;objects&nbsp;may&nbsp;break&nbsp;backwards&nbsp;compatibility&nbsp;if<br>
their&nbsp;serialized&nbsp;pickled&nbsp;form&nbsp;changes.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="builtins.html#object">object</a>:&nbsp;Any&nbsp;extra&nbsp;state&nbsp;to&nbsp;store&nbsp;in&nbsp;the&nbsp;module's&nbsp;state_dict</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-get_parameter"><strong>get_parameter</strong></a>(self, target: str) -&gt; 'Parameter'</dt><dd><span class="code">Return&nbsp;the&nbsp;parameter&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
See&nbsp;the&nbsp;docstring&nbsp;for&nbsp;``get_submodule``&nbsp;for&nbsp;a&nbsp;more&nbsp;detailed<br>
explanation&nbsp;of&nbsp;this&nbsp;method's&nbsp;functionality&nbsp;as&nbsp;well&nbsp;as&nbsp;how&nbsp;to<br>
correctly&nbsp;specify&nbsp;``target``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;Parameter<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;``get_submodule``&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Parameter:&nbsp;The&nbsp;Parameter&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;the&nbsp;target&nbsp;string&nbsp;references&nbsp;an&nbsp;invalid<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;or&nbsp;resolves&nbsp;to&nbsp;something&nbsp;that&nbsp;is&nbsp;not&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``nn.Parameter``</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-get_submodule"><strong>get_submodule</strong></a>(self, target: str) -&gt; 'Module'</dt><dd><span class="code">Return&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(16,&nbsp;33,&nbsp;kernel_size=(3,&nbsp;3),&nbsp;stride=(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(in_features=100,&nbsp;out_features=200,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;which&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;check&nbsp;whether&nbsp;or&nbsp;not&nbsp;we&nbsp;have&nbsp;the&nbsp;``linear``&nbsp;submodule,&nbsp;we<br>
would&nbsp;call&nbsp;``<a href="#TemperatureScalingEstimator-get_submodule">get_submodule</a>("net_b.linear")``.&nbsp;To&nbsp;check&nbsp;whether<br>
we&nbsp;have&nbsp;the&nbsp;``conv``&nbsp;submodule,&nbsp;we&nbsp;would&nbsp;call<br>
``<a href="#TemperatureScalingEstimator-get_submodule">get_submodule</a>("net_b.net_c.conv")``.<br>
&nbsp;<br>
The&nbsp;runtime&nbsp;of&nbsp;``get_submodule``&nbsp;is&nbsp;bounded&nbsp;by&nbsp;the&nbsp;degree<br>
of&nbsp;module&nbsp;nesting&nbsp;in&nbsp;``target``.&nbsp;A&nbsp;query&nbsp;against<br>
``named_modules``&nbsp;achieves&nbsp;the&nbsp;same&nbsp;result,&nbsp;but&nbsp;it&nbsp;is&nbsp;O(N)&nbsp;in<br>
the&nbsp;number&nbsp;of&nbsp;transitive&nbsp;modules.&nbsp;So,&nbsp;for&nbsp;a&nbsp;simple&nbsp;check&nbsp;to&nbsp;see<br>
if&nbsp;some&nbsp;submodule&nbsp;exists,&nbsp;``get_submodule``&nbsp;should&nbsp;always&nbsp;be<br>
used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;The&nbsp;submodule&nbsp;referenced&nbsp;by&nbsp;``target``<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-half"><strong>half</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-ipu"><strong>ipu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;IPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;IPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-load_state_dict"><strong>load_state_dict</strong></a>(
    self,
    state_dict: collections.abc.Mapping[str, typing.Any],
    strict: bool = True,
    assign: bool = False
)</dt><dd><span class="code">Copy&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into&nbsp;this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.<br>
&nbsp;<br>
If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;:attr:`assign`&nbsp;is&nbsp;``True``&nbsp;the&nbsp;optimizer&nbsp;must&nbsp;be&nbsp;created&nbsp;after<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;call&nbsp;to&nbsp;:attr:`load_state_dict`&nbsp;unless<br>
&nbsp;&nbsp;&nbsp;&nbsp;:func:`~torch.__future__.get_swap_module_params_on_conversion`&nbsp;is&nbsp;``True``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;&nbsp;&nbsp;&nbsp;assign&nbsp;(bool,&nbsp;optional):&nbsp;When&nbsp;set&nbsp;to&nbsp;``False``,&nbsp;the&nbsp;properties&nbsp;of&nbsp;the&nbsp;tensors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;current&nbsp;module&nbsp;are&nbsp;preserved&nbsp;whereas&nbsp;setting&nbsp;it&nbsp;to&nbsp;``True``&nbsp;preserves<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;properties&nbsp;of&nbsp;the&nbsp;Tensors&nbsp;in&nbsp;the&nbsp;state&nbsp;dict.&nbsp;The&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exception&nbsp;is&nbsp;the&nbsp;``requires_grad``&nbsp;field&nbsp;of&nbsp;:class:`~torch.nn.Parameter`s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;which&nbsp;the&nbsp;value&nbsp;from&nbsp;the&nbsp;module&nbsp;is&nbsp;preserved.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;any&nbsp;keys&nbsp;that&nbsp;are&nbsp;expected<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;missing&nbsp;from&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;keys&nbsp;that&nbsp;are&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expected&nbsp;by&nbsp;this&nbsp;module&nbsp;but&nbsp;present&nbsp;in&nbsp;the&nbsp;provided&nbsp;``state_dict``.<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;a&nbsp;parameter&nbsp;or&nbsp;buffer&nbsp;is&nbsp;registered&nbsp;as&nbsp;``None``&nbsp;and&nbsp;its&nbsp;corresponding&nbsp;key<br>
&nbsp;&nbsp;&nbsp;&nbsp;exists&nbsp;in&nbsp;:attr:`state_dict`,&nbsp;:meth:`load_state_dict`&nbsp;will&nbsp;raise&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;``RuntimeError``.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-modules"><strong>modules</strong></a>(self) -&gt; collections.abc.Iterator['Module']</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#TemperatureScalingEstimator-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-mtia"><strong>mtia</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;MTIA.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;the&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;MTIA&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-named_buffers"><strong>named_buffers</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool,&nbsp;optional):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;buffers&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#TemperatureScalingEstimator-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-named_children"><strong>named_children</strong></a>(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#TemperatureScalingEstimator-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-named_modules"><strong>named_modules</strong></a>(
    self,
    memo: Optional[set['Module']] = None,
    prefix: str = '',
    remove_duplicate: bool = True
)</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;memo:&nbsp;a&nbsp;memo&nbsp;to&nbsp;store&nbsp;the&nbsp;set&nbsp;of&nbsp;modules&nbsp;already&nbsp;added&nbsp;to&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix:&nbsp;a&nbsp;prefix&nbsp;that&nbsp;will&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate:&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated&nbsp;module&nbsp;instances&nbsp;in&nbsp;the&nbsp;result<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;not<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#TemperatureScalingEstimator-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-named_parameters"><strong>named_parameters</strong></a>(
    self,
    prefix: str = '',
    recurse: bool = True,
    remove_duplicate: bool = True
) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;remove_duplicate&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;remove&nbsp;the&nbsp;duplicated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;the&nbsp;result.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(str,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#TemperatureScalingEstimator-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-parameters"><strong>parameters</strong></a>(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</dt><dd><span class="code">Return&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#TemperatureScalingEstimator-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#TemperatureScalingEstimator-type">type</a>(param),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_backward_hook"><strong>register_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;deprecated&nbsp;in&nbsp;favor&nbsp;of&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.register_full_backward_hook`&nbsp;and<br>
the&nbsp;behavior&nbsp;of&nbsp;this&nbsp;function&nbsp;will&nbsp;change&nbsp;in&nbsp;future&nbsp;versions.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_buffer"><strong>register_buffer</strong></a>(
    self,
    name: str,
    tensor: Optional[torch.Tensor],
    persistent: bool = True
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;module's&nbsp;state.&nbsp;Buffers,&nbsp;by<br>
default,&nbsp;are&nbsp;persistent&nbsp;and&nbsp;will&nbsp;be&nbsp;saved&nbsp;alongside&nbsp;parameters.&nbsp;This<br>
behavior&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;setting&nbsp;:attr:`persistent`&nbsp;to&nbsp;``False``.&nbsp;The<br>
only&nbsp;difference&nbsp;between&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;and&nbsp;a&nbsp;non-persistent&nbsp;buffer<br>
is&nbsp;that&nbsp;the&nbsp;latter&nbsp;will&nbsp;not&nbsp;be&nbsp;a&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
:attr:`state_dict`.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor&nbsp;or&nbsp;None):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.&nbsp;If&nbsp;``None``,&nbsp;then&nbsp;operations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;run&nbsp;on&nbsp;buffers,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;buffer&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the&nbsp;module's&nbsp;:attr:`state_dict`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;(bool):&nbsp;whether&nbsp;the&nbsp;buffer&nbsp;is&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:attr:`state_dict`.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#TemperatureScalingEstimator-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_forward_hook"><strong>register_forward_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False,
    always_call: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``False``&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but&nbsp;it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on<br>
forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after&nbsp;:func:`forward`&nbsp;is&nbsp;called.&nbsp;The&nbsp;hook<br>
should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;``True``,&nbsp;the&nbsp;forward&nbsp;hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
``kwargs``&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function&nbsp;and&nbsp;be&nbsp;expected&nbsp;to&nbsp;return&nbsp;the<br>
output&nbsp;possibly&nbsp;modified.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;``True``,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;always_call&nbsp;(bool):&nbsp;If&nbsp;``True``&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;run&nbsp;regardless&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;whether&nbsp;an&nbsp;exception&nbsp;is&nbsp;raised&nbsp;while&nbsp;calling&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(
    self,
    hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]],
    *,
    prepend: bool = False,
    with_kwargs: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
&nbsp;<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;false&nbsp;or&nbsp;not&nbsp;specified,&nbsp;the&nbsp;input&nbsp;contains&nbsp;only<br>
the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.&nbsp;Keyword&nbsp;arguments&nbsp;won't&nbsp;be<br>
passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.&nbsp;The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the<br>
input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a&nbsp;single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the<br>
hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple&nbsp;if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned<br>
(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the<br>
following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
If&nbsp;``with_kwargs``&nbsp;is&nbsp;true,&nbsp;the&nbsp;forward&nbsp;pre-hook&nbsp;will&nbsp;be&nbsp;passed&nbsp;the<br>
kwargs&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.&nbsp;And&nbsp;if&nbsp;the&nbsp;hook&nbsp;modifies&nbsp;the<br>
input,&nbsp;both&nbsp;the&nbsp;args&nbsp;and&nbsp;kwargs&nbsp;should&nbsp;be&nbsp;returned.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have<br>
the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;args,&nbsp;kwargs)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;a&nbsp;tuple&nbsp;of&nbsp;modified&nbsp;input&nbsp;and&nbsp;kwargs<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user&nbsp;defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``forward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``forward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_forward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_kwargs&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;passed&nbsp;the&nbsp;kwargs<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;given&nbsp;to&nbsp;the&nbsp;forward&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_full_backward_hook"><strong>register_full_backward_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;a&nbsp;module<br>
are&nbsp;computed,&nbsp;i.e.&nbsp;the&nbsp;hook&nbsp;will&nbsp;execute&nbsp;if&nbsp;and&nbsp;only&nbsp;if&nbsp;the&nbsp;gradients&nbsp;with<br>
respect&nbsp;to&nbsp;module&nbsp;outputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following<br>
signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple(Tensor)&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;are&nbsp;tuples&nbsp;that&nbsp;contain&nbsp;the&nbsp;gradients<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs&nbsp;respectively.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;:attr:`grad_input`&nbsp;will&nbsp;only&nbsp;correspond&nbsp;to&nbsp;the&nbsp;inputs&nbsp;given<br>
as&nbsp;positional&nbsp;arguments&nbsp;and&nbsp;all&nbsp;kwarg&nbsp;arguments&nbsp;are&nbsp;ignored.&nbsp;Entries<br>
in&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for&nbsp;all&nbsp;non-Tensor<br>
arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;or&nbsp;outputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward``&nbsp;hooks&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_full_backward_pre_hook"><strong>register_full_backward_pre_hook</strong></a>(
    self,
    hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],
    prepend: bool = False
) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><span class="code">Register&nbsp;a&nbsp;backward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;for&nbsp;the&nbsp;module&nbsp;are&nbsp;computed.<br>
The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_output)&nbsp;-&gt;&nbsp;tuple[Tensor]&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_output`&nbsp;is&nbsp;a&nbsp;tuple.&nbsp;The&nbsp;hook&nbsp;should<br>
not&nbsp;modify&nbsp;its&nbsp;arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with<br>
respect&nbsp;to&nbsp;the&nbsp;output&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_output`&nbsp;in<br>
subsequent&nbsp;computations.&nbsp;Entries&nbsp;in&nbsp;:attr:`grad_output`&nbsp;will&nbsp;be&nbsp;``None``&nbsp;for<br>
all&nbsp;non-Tensor&nbsp;arguments.<br>
&nbsp;<br>
For&nbsp;technical&nbsp;reasons,&nbsp;when&nbsp;this&nbsp;hook&nbsp;is&nbsp;applied&nbsp;to&nbsp;a&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>,&nbsp;its&nbsp;forward&nbsp;function&nbsp;will<br>
receive&nbsp;a&nbsp;view&nbsp;of&nbsp;each&nbsp;Tensor&nbsp;passed&nbsp;to&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>.&nbsp;Similarly&nbsp;the&nbsp;caller&nbsp;will&nbsp;receive&nbsp;a&nbsp;view<br>
of&nbsp;each&nbsp;Tensor&nbsp;returned&nbsp;by&nbsp;the&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>'s&nbsp;forward&nbsp;function.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Modifying&nbsp;inputs&nbsp;inplace&nbsp;is&nbsp;not&nbsp;allowed&nbsp;when&nbsp;using&nbsp;backward&nbsp;hooks&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;raise&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;The&nbsp;user-defined&nbsp;hook&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prepend&nbsp;(bool):&nbsp;If&nbsp;true,&nbsp;the&nbsp;provided&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks&nbsp;on&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Otherwise,&nbsp;the&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``hook``&nbsp;will&nbsp;be&nbsp;fired&nbsp;after&nbsp;all&nbsp;existing&nbsp;``backward_pre``&nbsp;hooks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;this&nbsp;:class:`torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>`.&nbsp;Note&nbsp;that&nbsp;global<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``backward_pre``&nbsp;hooks&nbsp;registered&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:func:`register_module_full_backward_pre_hook`&nbsp;will&nbsp;fire&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;hooks&nbsp;registered&nbsp;by&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_load_state_dict_post_hook"><strong>register_load_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;after&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;incompatible_keys)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;``module``&nbsp;argument&nbsp;is&nbsp;the&nbsp;current&nbsp;module&nbsp;that&nbsp;this&nbsp;hook&nbsp;is&nbsp;registered<br>
on,&nbsp;and&nbsp;the&nbsp;``incompatible_keys``&nbsp;argument&nbsp;is&nbsp;a&nbsp;``NamedTuple``&nbsp;consisting<br>
of&nbsp;attributes&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``.&nbsp;``missing_keys``<br>
is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys&nbsp;and<br>
``unexpected_keys``&nbsp;is&nbsp;a&nbsp;``list``&nbsp;of&nbsp;``str``&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys.<br>
&nbsp;<br>
The&nbsp;given&nbsp;incompatible_keys&nbsp;can&nbsp;be&nbsp;modified&nbsp;inplace&nbsp;if&nbsp;needed.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;checks&nbsp;performed&nbsp;when&nbsp;calling&nbsp;:func:`load_state_dict`&nbsp;with<br>
``strict=True``&nbsp;are&nbsp;affected&nbsp;by&nbsp;modifications&nbsp;the&nbsp;hook&nbsp;makes&nbsp;to<br>
``missing_keys``&nbsp;or&nbsp;``unexpected_keys``,&nbsp;as&nbsp;expected.&nbsp;Additions&nbsp;to&nbsp;either<br>
set&nbsp;of&nbsp;keys&nbsp;will&nbsp;result&nbsp;in&nbsp;an&nbsp;error&nbsp;being&nbsp;thrown&nbsp;when&nbsp;``strict=True``,&nbsp;and<br>
clearing&nbsp;out&nbsp;both&nbsp;missing&nbsp;and&nbsp;unexpected&nbsp;keys&nbsp;will&nbsp;avoid&nbsp;an&nbsp;error.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_load_state_dict_pre_hook"><strong>register_load_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;to&nbsp;be&nbsp;run&nbsp;before&nbsp;module's&nbsp;:meth:`~nn.<a href="torch.nn.modules.module.html#Module">Module</a>.load_state_dict`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata,&nbsp;strict,&nbsp;missing_keys,&nbsp;unexpected_keys,&nbsp;error_msgs)&nbsp;-&gt;&nbsp;None&nbsp;&nbsp;#&nbsp;noqa:&nbsp;B950<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook&nbsp;(Callable):&nbsp;Callable&nbsp;hook&nbsp;that&nbsp;will&nbsp;be&nbsp;invoked&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loading&nbsp;the&nbsp;state&nbsp;dict.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_module"><strong>register_module</strong></a>(self, name: str, module: Optional[ForwardRef('Module')]) -&gt; None</dt><dd><span class="code">Alias&nbsp;for&nbsp;:func:`add_module`.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_parameter"><strong>register_parameter</strong></a>(
    self,
    name: str,
    param: Optional[torch.nn.parameter.Parameter]
) -&gt; None</dt><dd><span class="code">Add&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(str):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter&nbsp;or&nbsp;None):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``None``,&nbsp;then&nbsp;operations&nbsp;that&nbsp;run&nbsp;on&nbsp;parameters,&nbsp;such&nbsp;as&nbsp;:attr:`cuda`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;ignored.&nbsp;If&nbsp;``None``,&nbsp;the&nbsp;parameter&nbsp;is&nbsp;**not**&nbsp;included&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;module's&nbsp;:attr:`state_dict`.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_state_dict_post_hook"><strong>register_state_dict_post_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;post-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;state_dict,&nbsp;prefix,&nbsp;local_metadata)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;modify&nbsp;the&nbsp;``state_dict``&nbsp;inplace.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-register_state_dict_pre_hook"><strong>register_state_dict_pre_hook</strong></a>(self, hook)</dt><dd><span class="code">Register&nbsp;a&nbsp;pre-hook&nbsp;for&nbsp;the&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;method.<br>
&nbsp;<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;prefix,&nbsp;keep_vars)&nbsp;-&gt;&nbsp;None<br>
&nbsp;<br>
The&nbsp;registered&nbsp;hooks&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;perform&nbsp;pre-processing&nbsp;before&nbsp;the&nbsp;``state_dict``<br>
call&nbsp;is&nbsp;made.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-requires_grad_"><strong>requires_grad_</strong></a>(self: ~T, requires_grad: bool = True) -&gt; ~T</dt><dd><span class="code">Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
See&nbsp;:ref:`locally-disable-grad-doc`&nbsp;for&nbsp;a&nbsp;comparison&nbsp;between<br>
`.<a href="#TemperatureScalingEstimator-requires_grad_">requires_grad_</a>()`&nbsp;and&nbsp;several&nbsp;similar&nbsp;mechanisms&nbsp;that&nbsp;may&nbsp;be&nbsp;confused&nbsp;with&nbsp;it.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-set_extra_state"><strong>set_extra_state</strong></a>(self, state: Any) -&gt; None</dt><dd><span class="code">Set&nbsp;extra&nbsp;state&nbsp;contained&nbsp;in&nbsp;the&nbsp;loaded&nbsp;`state_dict`.<br>
&nbsp;<br>
This&nbsp;function&nbsp;is&nbsp;called&nbsp;from&nbsp;:func:`load_state_dict`&nbsp;to&nbsp;handle&nbsp;any&nbsp;extra&nbsp;state<br>
found&nbsp;within&nbsp;the&nbsp;`state_dict`.&nbsp;Implement&nbsp;this&nbsp;function&nbsp;and&nbsp;a&nbsp;corresponding<br>
:func:`get_extra_state`&nbsp;for&nbsp;your&nbsp;module&nbsp;if&nbsp;you&nbsp;need&nbsp;to&nbsp;store&nbsp;extra&nbsp;state&nbsp;within&nbsp;its<br>
`state_dict`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;(dict):&nbsp;Extra&nbsp;state&nbsp;from&nbsp;the&nbsp;`state_dict`</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-set_submodule"><strong>set_submodule</strong></a>(self, target: str, module: 'Module', strict: bool = False) -&gt; None</dt><dd><span class="code">Set&nbsp;the&nbsp;submodule&nbsp;given&nbsp;by&nbsp;``target``&nbsp;if&nbsp;it&nbsp;exists,&nbsp;otherwise&nbsp;throw&nbsp;an&nbsp;error.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``False``&nbsp;(default),&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``strict``&nbsp;is&nbsp;set&nbsp;to&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error&nbsp;if<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;submodule&nbsp;does&nbsp;not&nbsp;exist.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;let's&nbsp;say&nbsp;you&nbsp;have&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``&nbsp;that<br>
looks&nbsp;like&nbsp;this:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;text<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_b):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(net_c):&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(conv):&nbsp;Conv2d(3,&nbsp;3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(linear):&nbsp;Linear(3,&nbsp;3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
(The&nbsp;diagram&nbsp;shows&nbsp;an&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``&nbsp;``A``.&nbsp;``A``&nbsp;has&nbsp;a&nbsp;nested<br>
submodule&nbsp;``net_b``,&nbsp;which&nbsp;itself&nbsp;has&nbsp;two&nbsp;submodules&nbsp;``net_c``<br>
and&nbsp;``linear``.&nbsp;``net_c``&nbsp;then&nbsp;has&nbsp;a&nbsp;submodule&nbsp;``conv``.)<br>
&nbsp;<br>
To&nbsp;override&nbsp;the&nbsp;``Conv2d``&nbsp;with&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Linear``,&nbsp;you<br>
could&nbsp;call&nbsp;``<a href="#TemperatureScalingEstimator-set_submodule">set_submodule</a>("net_b.net_c.conv",&nbsp;nn.Linear(1,&nbsp;1))``<br>
where&nbsp;``strict``&nbsp;could&nbsp;be&nbsp;``True``&nbsp;or&nbsp;``False``<br>
&nbsp;<br>
To&nbsp;add&nbsp;a&nbsp;new&nbsp;submodule&nbsp;``Conv2d``&nbsp;to&nbsp;the&nbsp;existing&nbsp;``net_b``&nbsp;module,<br>
you&nbsp;would&nbsp;call&nbsp;``<a href="#TemperatureScalingEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1))``.<br>
&nbsp;<br>
In&nbsp;the&nbsp;above&nbsp;if&nbsp;you&nbsp;set&nbsp;``strict=True``&nbsp;and&nbsp;call<br>
``<a href="#TemperatureScalingEstimator-set_submodule">set_submodule</a>("net_b.conv",&nbsp;nn.Conv2d(1,&nbsp;1,&nbsp;1),&nbsp;strict=True)``,&nbsp;an&nbsp;AttributeError<br>
will&nbsp;be&nbsp;raised&nbsp;because&nbsp;``net_b``&nbsp;does&nbsp;not&nbsp;have&nbsp;a&nbsp;submodule&nbsp;named&nbsp;``conv``.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;target:&nbsp;The&nbsp;fully-qualified&nbsp;string&nbsp;name&nbsp;of&nbsp;the&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;look&nbsp;for.&nbsp;(See&nbsp;above&nbsp;example&nbsp;for&nbsp;how&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-qualified&nbsp;string.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;module:&nbsp;The&nbsp;module&nbsp;to&nbsp;set&nbsp;the&nbsp;submodule&nbsp;to.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;If&nbsp;``False``,&nbsp;the&nbsp;method&nbsp;will&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;create&nbsp;a&nbsp;new&nbsp;submodule&nbsp;if&nbsp;the&nbsp;parent&nbsp;module&nbsp;exists.&nbsp;If&nbsp;``True``,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;method&nbsp;will&nbsp;only&nbsp;attempt&nbsp;to&nbsp;replace&nbsp;an&nbsp;existing&nbsp;submodule&nbsp;and&nbsp;throw&nbsp;an&nbsp;error<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;the&nbsp;submodule&nbsp;doesn't&nbsp;already&nbsp;exist.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;``target``&nbsp;string&nbsp;is&nbsp;empty&nbsp;or&nbsp;if&nbsp;``module``&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;AttributeError:&nbsp;If&nbsp;at&nbsp;any&nbsp;point&nbsp;along&nbsp;the&nbsp;path&nbsp;resulting&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;``target``&nbsp;string&nbsp;the&nbsp;(sub)path&nbsp;resolves&nbsp;to&nbsp;a&nbsp;non-existent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute&nbsp;name&nbsp;or&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;that&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;``nn.<a href="torch.nn.modules.module.html#Module">Module</a>``.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-share_memory"><strong>share_memory</strong></a>(self: ~T) -&gt; ~T</dt><dd><span class="code">See&nbsp;:meth:`torch.Tensor.share_memory_`.</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-state_dict"><strong>state_dict</strong></a>(self, *args, destination=None, prefix='', keep_vars=False)</dt><dd><span class="code">Return&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;references&nbsp;to&nbsp;the&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
Parameters&nbsp;and&nbsp;buffers&nbsp;set&nbsp;to&nbsp;``None``&nbsp;are&nbsp;not&nbsp;included.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;returned&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;a&nbsp;shallow&nbsp;copy.&nbsp;It&nbsp;contains&nbsp;references<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;module's&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;``<a href="#TemperatureScalingEstimator-state_dict">state_dict</a>()``&nbsp;also&nbsp;accepts&nbsp;positional&nbsp;arguments&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;``destination``,&nbsp;``prefix``&nbsp;and&nbsp;``keep_vars``&nbsp;in&nbsp;order.&nbsp;However,<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;being&nbsp;deprecated&nbsp;and&nbsp;keyword&nbsp;arguments&nbsp;will&nbsp;be&nbsp;enforced&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;future&nbsp;releases.<br>
&nbsp;<br>
..&nbsp;warning::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;avoid&nbsp;the&nbsp;use&nbsp;of&nbsp;argument&nbsp;``destination``&nbsp;as&nbsp;it&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;designed&nbsp;for&nbsp;end-users.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;destination&nbsp;(dict,&nbsp;optional):&nbsp;If&nbsp;provided,&nbsp;the&nbsp;state&nbsp;of&nbsp;module&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;updated&nbsp;into&nbsp;the&nbsp;dict&nbsp;and&nbsp;the&nbsp;same&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;an&nbsp;``OrderedDict``&nbsp;will&nbsp;be&nbsp;created&nbsp;and&nbsp;returned.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``None``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str,&nbsp;optional):&nbsp;a&nbsp;prefix&nbsp;added&nbsp;to&nbsp;parameter&nbsp;and&nbsp;buffer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;names&nbsp;to&nbsp;compose&nbsp;the&nbsp;keys&nbsp;in&nbsp;state_dict.&nbsp;Default:&nbsp;``''``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;keep_vars&nbsp;(bool,&nbsp;optional):&nbsp;by&nbsp;default&nbsp;the&nbsp;:class:`~torch.Tensor`&nbsp;s<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returned&nbsp;in&nbsp;the&nbsp;state&nbsp;dict&nbsp;are&nbsp;detached&nbsp;from&nbsp;autograd.&nbsp;If&nbsp;it's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;to&nbsp;``True``,&nbsp;detaching&nbsp;will&nbsp;not&nbsp;be&nbsp;performed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default:&nbsp;``False``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+SKIP("undefined&nbsp;vars")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#TemperatureScalingEstimator-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><span class="code">Move&nbsp;and/or&nbsp;cast&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TemperatureScalingEstimator-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TemperatureScalingEstimator-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TemperatureScalingEstimator-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TemperatureScalingEstimator-to">to</a>(memory_format=torch.channels_last)<br>
&nbsp;&nbsp;&nbsp;:noindex:<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;:attr:`dtype`\&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;or&nbsp;complex&nbsp;dtype&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_format&nbsp;(:class:`torch.memory_format`):&nbsp;the&nbsp;desired&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format&nbsp;for&nbsp;4D&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module&nbsp;(keyword<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;argument)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+IGNORE_WANT("non-deterministic")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TemperatureScalingEstimator-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;#&nbsp;xdoctest:&nbsp;+REQUIRES(env:TORCH_DOCTEST_CUDA1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TemperatureScalingEstimator-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TemperatureScalingEstimator-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2,&nbsp;bias=None).<a href="#TemperatureScalingEstimator-to">to</a>(torch.cdouble)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.3741+0.j,&nbsp;&nbsp;0.2382+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;0.5593+0.j,&nbsp;-0.4443+0.j]],&nbsp;dtype=torch.complex128)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear(torch.ones(3,&nbsp;2,&nbsp;dtype=torch.cdouble))<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0.6122+0.j,&nbsp;0.1150+0.j]],&nbsp;dtype=torch.complex128)</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-to_empty"><strong>to_empty</strong></a>(
    self: ~T,
    *,
    device: Union[int, str, torch.device, NoneType],
    recurse: bool = True
) -&gt; ~T</dt><dd><span class="code">Move&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;specified&nbsp;device&nbsp;without&nbsp;copying&nbsp;storage.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;The&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;Whether&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;of&nbsp;submodules&nbsp;should<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;recursively&nbsp;moved&nbsp;to&nbsp;the&nbsp;specified&nbsp;device.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-train"><strong>train</strong></a>(self: ~T, mode: bool = True) -&gt; ~T</dt><dd><span class="code">Set&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;an&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;the&nbsp;documentation&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;i.e.,&nbsp;whether&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-type"><strong>type</strong></a>(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</dt><dd><span class="code">Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-xpu"><strong>xpu</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><span class="code">Move&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;XPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;XPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</span></dd></dl>

<dl><dt><a name="TemperatureScalingEstimator-zero_grad"><strong>zero_grad</strong></a>(self, set_to_none: bool = True) -&gt; None</dt><dd><span class="code">Reset&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters.<br>
&nbsp;<br>
See&nbsp;similar&nbsp;function&nbsp;under&nbsp;:class:`torch.optim.Optimizer`&nbsp;for&nbsp;more&nbsp;context.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;set_to_none&nbsp;(bool):&nbsp;instead&nbsp;of&nbsp;setting&nbsp;to&nbsp;zero,&nbsp;set&nbsp;the&nbsp;grads&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;:meth:`torch.optim.Optimizer.zero_grad`&nbsp;for&nbsp;details.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>T_destination</strong> = ~T_destination</dl>

<dl><dt><strong>call_super_init</strong> = False</dl>

<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor functions-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Functions</strong></td></tr>
    
<tr><td class="decor functions-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt><a name="-create_confidence_estimator"><strong>create_confidence_estimator</strong></a>(
    vocab_size: int = 32000,
    hidden_dim: int = 4096,
    symbolic_dim: int = 512,
    **kwargs
) -&gt; adapters.confidence_estimator.ConfidenceEstimator</dt><dd><span class="code">Create&nbsp;a&nbsp;confidence&nbsp;estimator&nbsp;with&nbsp;standard&nbsp;configuration</span></dd></dl>
 <dl><dt><a name="-create_llama_confidence_estimator"><strong>create_llama_confidence_estimator</strong></a>(**kwargs) -&gt; adapters.confidence_estimator.ConfidenceEstimator</dt><dd><span class="code">Create&nbsp;confidence&nbsp;estimator&nbsp;optimized&nbsp;for&nbsp;Llama&nbsp;models</span></dd></dl>
</td></tr></table><p>
<table class="section">
<tr class="decor data-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Data</strong></td></tr>
    
<tr><td class="decor data-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><strong>Dict</strong> = typing.Dict<br>
<strong>List</strong> = typing.List<br>
<strong>Optional</strong> = typing.Optional<br>
<strong>Tuple</strong> = typing.Tuple<br>
<strong>Union</strong> = typing.Union</td></tr></table>
</body></html>